[
  {
    "objectID": "posts/cmpm1/index.html",
    "href": "posts/cmpm1/index.html",
    "title": "Computational Physics for the Masses Part 1: Spinning and Sloshing in Atoms",
    "section": "",
    "text": "In the zeroth part, I mentioned that we will do some almost professional science stuff. But to draw an owl, you need to start with circles, which is something we’ll do today. I’ll show you what does it actually mean to solve a quantum-mechanical problem and what are the most basic mathematical tools to do it.\nTo demonstrate the use of these tools, I need some physical example. There are a few choices, the most common one being the particle in square potential well. A system like that is nice because people are intuitively more familiar with problems in real space, but there are some nuances that get swept under the rug in introductory materials and don’t become really obvious in a calculus-based approach. However, computers are generally bad at calculus and really good at algebra, so we’ll rely more on the matrix formulation of quantum mechanics, where the finer details of real-space representation and consequences of having a finite amount of memory would force me to feed you too much information in one sitting. Fortunately, there are other simple examples that work without silent assumptions and demonstrate the basics just as well - systems with finite, small amount of possible states like particle spin and angular momentum."
  },
  {
    "objectID": "posts/cmpm1/index.html#search-for-more-spaces-or-how-many-bits-does-it-take-to-store-the-concept-of-multiple-sine-functions",
    "href": "posts/cmpm1/index.html#search-for-more-spaces-or-how-many-bits-does-it-take-to-store-the-concept-of-multiple-sine-functions",
    "title": "Computational Physics for the Masses Part 1: Spinning and Sloshing in Atoms",
    "section": "Search for more space(s) or: How many bits does it take to store the concept of multiple sine functions?",
    "text": "Search for more space(s) or: How many bits does it take to store the concept of multiple sine functions?\nUnless you’re working on quantum computing, a single two-level state as shown above is quite boring. To give a little bit more structure to our system, we’ll look at something with more degrees of freedom: a single atom of Rubidium-87. Before we get to the specifics, we need to talk a bit about systems with multiple (coupled) degrees of freedom.\n\nWhat’s a \\(\\otimes\\)?\nIn the case we have shown before, there was only one degree of freedom of the system - the spin. Correspondingly, the Hamiltonian acted only on this degree of freedom and encompassed all the interesting dynamics. But this will not always be the case. You can easily imagine a situation, where there’s more to the it than just the dynamics of a single state variable: For example, you could have multiple spins sitting on a chain or lattice and interact together. How do we conveniently write down a Hamiltonian for a system like this? Well, the system is a sum of its parts, so it’s not irrational to try to do it like this:\n\\[\\hat{H} = \\left( \\sum_k \\hat{H}^{(k)} \\right) + \\hat{H}_{int}.\\]\nThe individual degrees of freedom are described by their individual (sub-)Hamiltonians \\(\\hat{H}^{(k)}\\) and, if we continue with the example of the chain of spins in magnetic field, \\(\\hat{H}^{(k)}\\) would be the spin Hamiltonian acting exclusively on the k-th spin. Operators corresponding to interaction between the spins (e.g. exchange interactions) are all hidden in \\(\\hat{H}_{int}\\) which will act on the system as a whole. Formally speaking, the ability to split the Hamiltonian so neatly hinges on the total Hilbert space having a tensor-product structure - in the context of our discussion in the previous part, the state of the whole system can be described by enumerating states of all it’s subsystems. This is not always the case and we’ll be dealing with situations where it all comes crashing to the ground in the future. But it’s going to work out fine this time, so let’s focus on how to implement it.\nIf the Hilbert space \\(H\\) can be factorized into subspaces corresponding to the N individual degrees of freedom as\n\\[ H = H^{(1)} \\otimes H^{(2)} \\otimes \\ldots \\otimes H^{(N)}, \\]\nwhere \\(\\otimes\\) is the tensor product, then a single sub-Hamiltonian can be constructed as\n\\[ \\hat{H}^{(k)} = \\mathbf{1}^{(1)} \\otimes \\mathbf{1}^{(2)} \\otimes \\ldots \\otimes \\mathbf{1}^{(k-1)} \\otimes \\hat{h}^{(k)} \\otimes \\mathbf{1}^{(k+1)} \\otimes \\ldots \\otimes \\mathbf{1}^{(N)}, \\]\nwhere \\(\\hat{h}^{(k)}\\) is the Hamiltonian of a single degree of freedom, e.g. \\(\\hat{h} = -\\mu_B g_S \\hat{S_z}B_z\\) for spins in longitudinal field. This extends to the basis of the whole system, which is constructed by using all the tensor products of the subspace basis function sets \\(\\{ \\left| \\phi_{i_k} \\right> ^{(k)} \\} _{i_k = 1} ^{n_k}\\), i.e.:\n\\[ \\left| \\phi_{i_1}, \\phi_{i_2}, \\ldots, \\phi_{i_N} \\right> = \\left| \\phi_{i_1} \\right>^{(1)} \\otimes \\left| \\phi_{i_2} \\right>^{(2)} \\otimes ... \\otimes \\left| \\phi_{i_N} \\right>^{(N)}.\\]\nProduct spaces can become really large quickly, as the number of elements in the basis goes as \\(\\prod_{k=1}^N n_k\\), with \\(n_k\\) being the number of elements of k-th subspace basis. This is one of the main reasons why I initially said that this direct approach will eventually fail - even for just a simple system like a spin chain (two basis vectors per subsystem), the basis for N spins has \\(\\mathrm{2^N}\\) elements, which is a scaling that gives computer scientists the heebie-jeebies.\nFollowing the same discussion as in Part 0, we can express the computer representation of a product state\n\\[ \\left| \\psi \\right> = \\left| \\psi_1 \\right>^{(1)} \\otimes \\left| \\psi_2 \\right>^{(2)} \\otimes \\ldots \\otimes \\left| \\psi_N \\right>^{(N)} = \\sum_{i_1 = 1}^{n_1} \\sum_{i_2 = 1}^{n_2}\\ldots\\sum_{i_n = 1}^{n_N} \\psi_{i_1,i_2,\\ldots,i_N} \\left| \\phi_{i_1},\\phi_{i_2},\\ldots,\\phi_{i_N} \\right>, \\]\nwhere \\(\\psi_{i_1,i_2,...,i_N}\\) is the vector stored in memory. To calculate this this product state in Julia, we use the Kronecker product by simply doing, e.g. ψ = kron(ψ1, ψ2, ψ3) to get a product of states ψ1, ψ2 and ψ3 (you can use as many states as you need, the kron function is variadic). The operators acting on the full Hilbert space will follow the same logic:\n\\[ \\hat{A} = \\hat{a}_1^{(1)} \\otimes \\hat{a}_2^{(2)} \\otimes \\ldots \\otimes \\hat{a}_N^{(N)} = \\sum_{i_1=1}^{n_1}\\sum_{j_1=1}^{n_1} \\sum_{i_2=1}^{n_2}\\sum_{j_2=1}^{n_2} \\ldots \\sum_{i_N=1}^{n_N}\\sum_{j_N=1}^{n_N} \\left[ a_{i_1j_1}^{(1)} a_{i_2j_2}^{(2)} \\ldots a_{i_Nj_N}^{(N)} \\right] \\left| \\phi_{i_1},\\phi_{i_2},\\ldots,\\phi_{i_N} \\right> \\left< \\phi_{i_1},\\phi_{i_2},\\ldots,\\phi_{i_N} \\right| = \\\\ \\sum_{i_1=1}^{n_1}\\sum_{j_1=1}^{n_1} \\sum_{i_2=1}^{n_2}\\sum_{j_2=1}^{n_2} \\ldots \\sum_{i_N=1}^{n_N}\\sum_{j_N=1}^{n_N} a_{i_1j_1 i_2j_2 \\ldots i_Nj_N} \\left| \\phi_{i_1},\\phi_{i_2},\\ldots,\\phi_{i_N} \\right> \\left< \\phi_{i_1},\\phi_{i_2},\\ldots,\\phi_{i_N} \\right|, \\]\nwhere the monstrosity \\(a_{i_1j_1 i_2j_2 \\ldots i_Nj_N}\\) is the operator matrix used in calculations. Writing so many summation symbols and indices in a single expression has already cost me probably half of my readers, so before I lose the rest of you: To make a full operator matrix A out of subspace operators A1, A2 and A3, you use the same function as before, i.e. A = kron(A1, A2, A3). It’s just that easy.\nThis should give us all that’s necessary to do the first real-world physics.\n\n\nThe not-so-curious case of \\(\\mathrm{^{87}Rb}\\)\nWe started with spin Hamiltonians because they are easy to work with and demonstrate basic concepts on, but that doesn’t mean that there’s no use for these systems in the wild. There’s a lot of applications in AMO physics that use transitions between atomic angular momentum or spin states, such as atomic clocks and these transitions in Caesium are even used as a standard which defines the length of a second. While less precise than Caesium, the Rubidium-87 standard is much more common and used in applications that require precise timing, such as GPS satellites or cell towers, and we’ll look at its spin structure.\nRubidium is an alkali metal with 36 core electrons that completely fill the principal energy levels and don’t contribute to the atom’s spin or angular momentum (a fully filled electron shell is isotropic) and a single valence electron that has zero angular momentum and spin of 1/2 (that is, in the electronic ground-state). The slightly radioactive (with half-life of 49 billion years, so only very slightly) isotope \\(\\mathrm{^{87}Rb}\\) has a nuclear spin of 3/2. These fully define the spin system that we’ll look at and, from the discussion in the preceding paragraphs, the basis of our Hamiltonian will be a tensor product basis of spin-3/2, spin-1/2 and spin-0 spaces. The operators act only on their respective subspaces, so their action on the total space is defined really easily: \\(\\hat{\\mathbf{I}} \\mapsto \\hat{\\mathbf{I}} \\otimes \\hat{\\mathbf{1}} \\otimes \\hat{\\mathbf{1}}\\), \\(\\hat{\\mathbf{S}} \\mapsto \\hat{\\mathbf{1}} \\otimes \\hat{\\mathbf{S}} \\otimes \\hat{\\mathbf{1}}\\) and \\(\\hat{\\mathbf{L}} \\mapsto \\hat{\\mathbf{1}} \\otimes \\hat{\\mathbf{1}} \\otimes \\hat{\\mathbf{L}}\\) for the nuclear spin, electron spin and electron orbital angular momentum, respectively (total electron angular momentum is \\(\\hat{\\mathbf{J}} = \\hat{\\mathbf{S}} + \\hat{\\mathbf{L}}\\)). We’ll limit ourselves to only spin dipole-dipole interactions (see here) which means that the Hamiltonian in longitudinal field looks like this:\n\\[ \\hat{H}_0 = h A_{hfs}\\hat{\\mathbf{I}} \\cdot \\hat{\\mathbf{J}} - \\mu_B B_Z \\left( g_I \\hat{I}_z + g_S \\hat{S}_z + g_L \\hat{L}_z \\right) ,\\]\nwhere \\(A_{hfs}\\) is the spin-spin coupling constant for \\(\\mathrm{^{87}Rb}\\) ground-state, \\(g_I\\), \\(g_S\\) and \\(g_L\\) the g-factors of the various forms of angular momentum in the system and \\(h\\) the Planck’s constant (equal to 1 in our units). Putting that into Julia code is as easy as writing out the individual operator components and summing/multiplying them (you’ll see why I give it the index 0 momentarily):\n\nfunction H0(bz)\n    ahfs = 3417.341305; # spin-spin coupling const for ⁸⁷Rb\n    gS = -2.00231930436; # Electron g-factor\n    gI = 0.0009951414; # Nuclear g-factor\n    gL = -0.99999369; # Electron orbital g-factor\n    μB = 1.3996245; # Bohr magneton in ħ*MHz/G\n    \n    # Nuclear spin\n    Ix = kron(sx(3/2), sid(1/2), sid(0));\n    Iy = kron(sy(3/2), sid(1/2), sid(0));\n    Iz = kron(sz(3/2), sid(1/2), sid(0));\n    # Electron spin\n    Sx = kron(sid(3/2), sx(1/2), sid(0));\n    Sy = kron(sid(3/2), sy(1/2), sid(0));\n    Sz = kron(sid(3/2), sz(1/2), sid(0));\n    # Electron angular momentum\n    Lx = kron(sid(3/2), sid(1/2), sx(0));\n    Ly = kron(sid(3/2), sid(1/2), sy(0));\n    Lz = kron(sid(3/2), sid(1/2), sz(0));\n    \n    # Total electron angular momentum\n    Jx = Sx .+ Lx;\n    Jy = Sy .+ Ly;\n    Jz = Sz .+ Lz;\n    \n    so = ahfs .* (Ix * Jx .+ Iy * Jy .+ Iz * Jz);\n    sb = @. μB * bz * (gI * Iz + gS * Sz + gL * Lz);\n    so .- sb\nend;\n\nHere is where the spin identity operators sid come into play. Their only purpose is to generate an identity matrix that has the same size as a non-trivial operator acting on the subspace. It’s also worth pointing out that the electron angular momentum \\(\\hat{\\mathbf{L}}\\) has only one state, which means that it doesn’t actually contribute to anything and it’s here just for completeness sake. We can take it for a ride in the same way as with just the bare spin before:\n\nb_range = 0:8000\nenergies = vcat((eigvals(H0(b))' for b in b_range)...)\nplot(b_range, energies, xlabel=\"B [G]\", ylabel=\"E [MHz]\", leg=false)\n\n\n\n\nNow this finally gives us something to talk about. First: yes, it does actually look exactly as it’s supposed to. Second: What the hell is happening with the energy levels? Not knowing the details, you would expect to zero-field state to be just a eight-fold degenerate ground state of all the individual spin states living in peace and harmony. But it’s not. Instead you see that, even at zero field, there are two separate energy levels. This is because the angular momenta in the atom are coupled (through the \\(\\hat{\\mathbf{I}}\\cdot\\hat{\\mathbf{J}}\\) term). Since nuclear and electronic spins (or angular momenta, whatever) are of length 3/2 and 1/2, the full system can only be in a state with total atom spin \\(\\hat{\\mathbf{F}} = \\hat{\\mathbf{I}} + \\hat{\\mathbf{J}}\\) of either F = 1 (a three-fold degenerate state), or F = 2 (with five-fold degeneracy). This forms the famous hyperfine structure. To confirm the assignment, we can take the energy eigenstates \\(\\{ \\left| \\varepsilon_i \\right> \\}_{i=1}^{8}\\) of the Hamiltonian and check if \\(\\left< \\varepsilon_i \\right| \\hat{F}_z \\left| \\varepsilon_j \\right>\\) and \\(\\left< \\varepsilon_i \\right| \\hat{F}^2 \\left| \\varepsilon_j \\right>\\) are diagonal:\n\nevecs = eigvecs(H0(0.0)) # Energy eigenvectors\n\nfunction Fz()\n    # Nuclear spin\n    Iz = kron(sz(3/2), sid(1/2), sid(0));\n    # Electron spin\n    Sz = kron(sid(3/2), sz(1/2), sid(0));\n    # Electron angular momentum\n    Lz = kron(sid(3/2), sid(1/2), sz(0));\n    \n    # Total electron angular momentum\n    Jz = Sz .+ Lz;\n    \n    # Total atomic angular momentum\n    Iz .+ Jz\nend\n\nfunction F2()\n    # Nuclear spin\n    Ix = kron(sx(3/2), sid(1/2), sid(0));\n    Iy = kron(sy(3/2), sid(1/2), sid(0));\n    Iz = kron(sz(3/2), sid(1/2), sid(0));\n    # Electron spin\n    Sx = kron(sid(3/2), sx(1/2), sid(0));\n    Sy = kron(sid(3/2), sy(1/2), sid(0));\n    Sz = kron(sid(3/2), sz(1/2), sid(0));\n    # Electron angular momentum\n    Lx = kron(sid(3/2), sid(1/2), sx(0));\n    Ly = kron(sid(3/2), sid(1/2), sy(0));\n    Lz = kron(sid(3/2), sid(1/2), sz(0));\n    \n    # Total electron angular momentum\n    Jx = Sx .+ Lx;\n    Jy = Sy .+ Ly;\n    Jz = Sz .+ Lz;\n    \n    # Total atomic angular momentum\n    Fx = Ix .+ Jx\n    Fy = Iy .+ Jy\n    Fz = Iz .+ Jz\n    \n    Fx*Fx .+ Fy*Fy .+ Fz*Fz\nend;\n\nI’ll cheat a little bit and take only the real part of the eigenvectors (the imaginary part in this case is zero) and get rid floating-point arithmetic errors so that the matrices look pretty. The \\(\\hat{F}_z\\) eigenvalues are out of order because the ordering of the degenerate energy eigenstates is arbitrary.\n\nInt.(round.(real.(transpose(evecs) * F2() * evecs)))\n\n8×8 Array{Int64,2}:\n 2  0  0  0  0  0  0  0\n 0  2  0  0  0  0  0  0\n 0  0  2  0  0  0  0  0\n 0  0  0  6  0  0  0  0\n 0  0  0  0  6  0  0  0\n 0  0  0  0  0  6  0  0\n 0  0  0  0  0  0  6  0\n 0  0  0  0  0  0  0  6\n\n\n\nInt.(round.(real.(transpose(evecs) * Fz() * evecs)))\n\n8×8 Array{Int64,2}:\n 1  0   0  0  0  0   0   0\n 0  0   0  0  0  0   0   0\n 0  0  -1  0  0  0   0   0\n 0  0   0  1  0  0   0   0\n 0  0   0  0  0  0   0   0\n 0  0   0  0  0  2   0   0\n 0  0   0  0  0  0  -1   0\n 0  0   0  0  0  0   0  -2\n\n\nAs you can see, the Hamiltonian does, indeed, commute with the atomic angular momentum operators and we can label the states by the atomic angular momentum quantum numbers as \\(\\left| f, m_f \\right>\\). This approximately holds also for small fields, where the three \\(\\left| 1, m_1 \\right>\\) and five \\(\\left| 2, m_2 \\right>\\) states Zeeman split, with the field dependence of the energy shifts being approximately linear with two different effective g-factors. At high fields, we cannot do this anymore and we need identify the eigenstates of the Hamiltonian with states \\(\\left|m_I, m_J \\right> = \\left|\\frac{3}{2}, m_I \\right> \\otimes \\left| \\frac{1}{2}, m_J \\right>\\) (the author is too lazy to do it, so the confirmation of this is, again, left as a exercise for the reader).\nThis is as far as we’ll go with the discussion on static ground states. If you feel like you had enough and need a break, now is probably the time to do it. Otherwise, let’s move on to the other promise of this part’s title, which implies also some dynamics.\n\n\nProgramming the prime mover\nCalculating properties of the static ground state was easy because it amounted to just finding the eigenstates of a static Hamiltonian. But dynamics are a bit more tricky. What we have to solve is the time-dependent Schrödinger equation:\n\\[ i\\hbar \\frac{d}{dt} \\left|\\psi(t) \\right> = \\hat{H}(t) \\left|\\psi(t) \\right>. \\]\nIf I were a theorist, I could say that the solution is easy and just write it down as\n\\[ \\left|\\psi(t) \\right> = \\hat{U}(t_0, t) \\left|\\psi(t_0) \\right>, \\]\nWhere \\(\\hat{U}(t_0, t)\\) is called the propagator, and just by skimming the Wikipedia article, you can probably tell that this “solution” is not too useful for us. The propagator, which is in this case the time evolution operator, has to have certain properties - it needs to conserve probability, meaning that the system always has to be in some state, and it has to be composable, i.e. \\(\\hat{U}(t_0, t_2) = \\hat{U}(t_0, t_1)\\hat{U}(t_1, t_2)\\). By accepting these demands and following the ritualistic practices of mathematics, you can arrive at a more specific form:\n\\[ \\hat{U}(t_0, t) = \\exp{\\left[ -\\frac{i}{\\hbar} \\int_{t_0}^t d\\tau \\hat{H}(\\tau) \\right]}. \\]\nThe question immediately becomes, how does one do an exponent of a operator? Ehh, you don’t really. What you can do is a series expansion, which in this case gives an infinite series of operators called the Dyson series:\n\\[ \\hat{U}(t_0, t) = \\hat{\\mathbf{1}} - \\frac{i}{\\hbar} \\int_{t_0}^{t}d\\tau \\hat{H}(\\tau_1) - \\frac{1}{\\hbar^2} \\int_{t_0}^{t}d\\tau\\int_{t_0}^{\\tau}d\\tau_1\\hat{H}(\\tau)\\hat{H}(\\tau_1) + \\frac{i}{\\hbar^3} \\int_{t_0}^{t}d\\tau\\int_{t_0}^{\\tau}d\\tau_1\\int_{t_0}^{\\tau_1}d\\tau_2\\hat{H}(\\tau)\\hat{H}(\\tau_1)\\hat{H}(\\tau_2) + \\ldots. \\]\nA series not dissimilar to this one is what’s encoded in Feynman diagrams, but that’s something the future us will work with. Alternatively, one can use the Magnus expansion:\n\\[ \\hat{U}(t_0, t_1) = \\exp{\\left[ \\sum_k \\hat{\\Omega}_k(t_0,t) \\right]}, \\]\nwhere\n\\[ \\hat{\\Omega}_1(t_0,t) = -\\frac{i}{\\hbar} \\int_{t_0}^t d\\tau \\hat{H}(\\tau), \\\\\n\\hat{\\Omega}_2(t_0,t) = -\\frac{1}{2\\hbar^2} \\int_{t_0}^t d\\tau \\int_{t_0}^{\\tau} d\\tau_1 \\left[ \\hat{H}(\\tau), \\hat{H}(\\tau_1) \\right], \\\\\n\\hat{\\Omega}_3(t_0,t) = -\\frac{1}{6\\hbar^3} \\int_{t_0}^t d\\tau \\int_{t_0}^{\\tau} d\\tau_1 \\int_{t_0}^{\\tau_1} d\\tau_2 \\left[ \\hat{H}(\\tau), [\\hat{H}(\\tau_1), \\hat{H}(\\tau_2)] \\right] + \\left[ \\hat{H}(\\tau_2), [\\hat{H}(\\tau_1), \\hat{H}(\\tau)] \\right], \\\\\n\\ldots \\]\nThis expansion shows, that the time propagator simplifies by a considerable amount, if the Hamiltonian commutes with itself at different times, \\(\\left[\\hat{H}(t), \\hat{H}(t') \\right] = 0\\), which leaves you with just the \\(\\hat{\\Omega}_1(t_0, t)\\) term and that becomes even easier if the Hamiltonian itself is time-independent. But don’t get your hopes too high up - most interesting problems aren’t like this.\nUntil now, I have been just adding integrals and operators to somehow express the state in a future time, but that doesn’t bring us much closer to actually solving the Schrödinger equation in a concrete manner. Let’s do that numerically, first, with a time-independent basis. This means that the basis \\(\\{ \\left| \\phi_i \\right> \\}\\) is fixed and it’s the operator and wavefunction expansion coefficients that change in time. By doing that, the Schrödinger equation takes the form\n\\[ i\\hbar \\sum_i \\frac{d}{dt} \\psi_i(t) \\left| \\phi_i \\right> = \\sum_{jk} H_{jk}(t)\\psi_k(t) \\left| \\phi_j \\right>, \\]\nwhich, after multiplying from the left by \\(\\left< \\phi_m \\right|\\) yields\n\\[ i\\hbar\\frac{d}{dt} \\psi_m(t) = \\sum_k H_{mk}(t)\\psi_k(t), \\]\nwhere \\(\\psi_{m,k}\\) and \\(H_{mk}\\) are just regular complex numbers (the basis is orthogonal, so \\(\\left< \\phi_i | \\phi_j \\right> = \\delta_{ij}\\)). This coupled system of differential equations, which we can rewrite as a vector equation \\(i\\hbar \\frac{d}{dt}\\vec{\\psi}(t) = \\mathbf{H}(t)\\cdot \\vec{\\psi}\\), can be solved on the computer by any method or library you desire.\nThere is, however, a more convenient way to represent the problem in our case: the interaction picture. In this case, we split the Hamiltonian into a time-independent part and a time-dependent part containing the interactions that cause system dynamics: \\(\\hat{H}(t) = \\hat{H}_0 + \\hat{H}_1(t)\\) (now you understand my choice of indexing the hyperfine Hamiltonian with 0). If we can diagonalize the time-independent Hamiltonian, i.e. find the eigenfunction basis\n\\[ \\hat{H}_0 \\left| \\phi_i \\right> = E_i \\left| \\phi_i \\right>, \\]\nWe can use it come up with a time-dependent basis in the form\n\\[ \\left| \\phi_i(t) \\right> = e^{-\\frac{i E_i}{\\hbar}t} \\left| \\phi_i \\right>. \\]\n(be careful not to confuse the complex \\(i\\) with the subscript index i)\nIn this basis, the time-dependent Schrödinger equation is\n\\[ i\\hbar \\sum_i \\frac{d}{dt}\\psi_i e^{-\\frac{i E_i}{\\hbar}t} \\left| \\phi_i \\right> = \\sum_j \\psi_j e^{-\\frac{i E_j}{\\hbar}t} \\hat{H}_1(t) \\left| \\phi_j \\right> \\]\nwhich, again, by doing left multiplication by \\(\\left< \\phi_k \\right|\\) becomes\n\\[ i\\hbar\\frac{d}{dt}\\psi_k(t) = \\sum_j \\psi_j(t)e^{-\\frac{i (E_j - E_k)}{\\hbar}t} \\left< \\phi_k \\right| \\hat{H}_1(t) \\left| \\phi_j \\right>. \\]\nThis is the same as the equation in time-independent basis, but with the dynamics encoded in the matrix \\(H_{mk} \\equiv H_{ij} = e^{-\\frac{i (E_i - E_j)}{\\hbar}t} \\left< \\phi_i \\right| \\hat{H}_1 \\left| \\phi_j \\right>\\).\n\n\nSeeing the light\nArmed with the knowledge of how to solve time-dependent problems, we can check what happens if the Rubidium atom is coupled to a oscillating magnetic field. The oscillating magnetic field can be also the magnetic component of a traveling electromagnetic wave, so we’re effectively looking at the behavior of the atom under light illumination. As per previous discussion, we’ll write down the Hamiltonian as a sum of the time-independent part (which is the hyperfine Hamiltonian that we used previously) and the oscillating perturbation part:\n\\[ \\hat{H}(t) = \\hat{H}_0 + \\cos(\\omega t)\\hat{H}_1 = h A_{hfs}\\hat{\\mathbf{I}} \\cdot \\hat{\\mathbf{J}} - \\mu_B B_Z \\left( g_I \\hat{I}_z + g_S \\hat{S}_z + g_L \\hat{L}_z \\right) - cos{( \\omega t)} \\times \\mu_B \\vec{B}_{ac} \\cdot \\left( g_I \\hat{\\mathbf{I}} + g_S \\hat{\\mathbf{S}} + g_L \\hat{\\mathbf{L}} \\right), \\]\nwhere \\(\\vec{B}_{ac}\\) is the magnetic field of the EM wave with angular frequency \\(\\omega\\). Unfortunately, \\(\\left[ \\hat{H}(t), \\hat{H}(t') \\right] \\neq 0\\), so there’s very little hope for us to find an analytic solution to this problem (AFAIK, it doesn’t exist) and the numerical solution that we’ll attempt here is about as good as it gets. We already have the eigenstates of the Hamiltonian \\(\\hat{H}_0\\), so we can use them to construct the set of equations that we need to solve:\n\\[ i\\hbar\\frac{d}{dt}\\psi_i(t)=\\sum_{j=1}^8 \\psi_j(t) e^{\\frac{-i(E_j - E_i)}{\\hbar}t} cos{(\\omega t)}\\left< \\phi_i \\right| \\hat{H}_1 \\left| \\phi_j \\right>\n= \\frac{1}{2} \\sum_{j=1}^8 \\psi_j(t) \\left[ e^{ \\left( \\frac{E_j - E_i}{\\hbar} - \\omega \\right) t} + e^{ \\left( \\frac{E_i - E_j}{\\hbar} - \\omega \\right) t} \\right] T_{ij}. \\]\nIn the second half of the equation, I used the Euler’s formula to rewrite cosine as a complex exponential and defined the transition matrix \\(T_{ij} \\equiv \\left< \\phi_i \\right| \\hat{H}_1 \\left| \\phi_j \\right>\\). This matrix describes how the different states couple to each other through the action of the oscillating magnetic field. One way to put this into code would be like this:\n\nfunction H1(bac)\n    gS = -2.00231930436; # Electron g-factor\n    gI = 0.0009951414; # Nuclear g-factor\n    gL = -0.99999369; # Electron orbital g-factor\n    μB = 1.3996245; # Bohr magneton in ħ*MHz/G\n    \n    # AC magnetic field components\n    bx = bac[1];\n    by = bac[2];\n    bz = bac[3];\n    \n    # Nuclear spin\n    Ix = kron(sx(3/2), sid(1/2), sid(0));\n    Iy = kron(sy(3/2), sid(1/2), sid(0));\n    Iz = kron(sz(3/2), sid(1/2), sid(0));\n    # Electron spin\n    Sx = kron(sid(3/2), sx(1/2), sid(0));\n    Sy = kron(sid(3/2), sy(1/2), sid(0));\n    Sz = kron(sid(3/2), sz(1/2), sid(0));\n    # Electron angular momentum\n    Lx = kron(sid(3/2), sid(1/2), sx(0));\n    Ly = kron(sid(3/2), sid(1/2), sy(0));\n    Lz = kron(sid(3/2), sid(1/2), sz(0));\n    \n    sb = @. gS * (bx * Sx + by * Sy + bz * Sz);\n    ib = @. gI * (bx * Ix + by * Iy + bz * Iz);\n    lb = @. gL * (bx * Lx + by * Ly + bz * Lz);\n    \n    @. -μB * (sb + ib + lb) \nend\n\nfunction T(bdc, bac)\n    evs = eigvecs(H0(bdc))\n    transpose(evs) * H1(bac) * evs\nend\n\nfunction Hij(ω, t, bdc, bac)\n    hbar = 1/(2*π)\n    \n    es = eigvals(H0(bdc))\n    n = length(es)\n    δE(i, j) = ((es[i] - es[j]) / hbar - ω) * t\n    mat = [exp(-1im * δE(j, i)) + exp(1im * δE(i, j)) for i in 1:n, j in 1:n] .* T(bdc, bac)\n    @. mat / 1im / hbar / 2.0\nend;\n\nThe matrix Hij absorbs all the constants, so that the differential equation can be written as \\(\\frac{d}{dt}\\vec{\\psi}(t) = H_{ij} \\cdot \\vec{\\psi}(t)\\) - this makes the code a little bit more legible when we feed it to the ODE solver.\nSpeaking of solvers, now is the time to discuss some performance considerations and profiling. No matter what library you use, the main part of what the solver is going to do is discretize the time and repeatedly apply the operator Hij to the solution vector (or initial condition) psi (psi0). The time step needs to be small and the number of operator applications will be big, easily tens of thousands, so you want the evaluation of the function Hij to be fast. To check how fast does a code run in Julia, you can use the @time macro. Because of the JIT compilation, it will be slow the first time you evaluate a function, so I recommend doing it at least twice to avoid timing also the compilation step. When we check the speed of our first attempt\n\n@time Hij(1000, 0.1, 4, [0.1; 0; 0]);\n\n  0.000164 seconds (368 allocations: 107.438 KiB)\n\n\nWe see that it takes ~160 us to run - not good. The number of memory allocations is bad too. When it takes 368 allocations to calculate the components of a 8x8 matrix, you usually know something went wrong.\nJulia achieves it’s high performance by (among other things) aggressively monomorphizing functions through multiple dispatch. This means that if you have a generic function f(x), it compiles down to machine code that’s separate for each type of x used, i.e. f(x::Int64) and f(x::Float64) can have two different definitions and, even if they don’t, things like basic arithmetic dispatches too, so they will become separate functions under the hood. But if the compiler cannot infer the type of your variables at compile-time, or the types might change (type instability), it has to fall back on abstract types, which will cause a lot of allocations and inhibit a lot of optimization steps. In a dynamic language like Julia, type instability is hard to avoid and sometimes doesn’t really matter, but you should do your best to get rid of it in performance-critical parts. To check for this problem, there’s another macro called @code_warntype, which will print the inferred types (or lack thereof):\n\n@code_warntype Hij(1000, 0.1, 4, [0.1; 0; 0])\n\nVariables\n  #self#::Core.Compiler.Const(Hij, false)\n  ω::Int64\n  t::Float64\n  bdc::Int64\n  bac::Array{Float64,1}\n  #9::getfield(Main, Symbol(\"##9#11\")){_A} where _A\n  hbar::Float64\n  es::Union{Array{Complex{Float64},1}, Array{Float64,1}}\n  n::Int64\n  δE::getfield(Main, Symbol(\"#δE#10\")){Int64,Float64,Float64,_A} where _A\n  mat::Any\n\nBody::Any\n1 ─ %1  = (2 * Main.π)::Core.Compiler.Const(6.283185307179586, false)\n│         (hbar = 1 / %1)\n│   %3  = Main.H0(bdc)::Array{Complex{Float64},2}\n│         (es = Main.eigvals(%3))\n│         (n = Main.length(es))\n│   %6  = Main.:(#δE#10)::Core.Compiler.Const(getfield(Main, Symbol(\"#δE#10\")), false)\n│   %7  = Core.typeof(ω)::Core.Compiler.Const(Int64, false)\n│   %8  = Core.typeof(t)::Core.Compiler.Const(Float64, false)\n│   %9  = Core.typeof(hbar::Core.Compiler.Const(0.15915494309189535, false))::Core.Compiler.Const(Float64, false)\n│   %10 = Core.typeof(es)::Union{Type{Array{Complex{Float64},1}}, Type{Array{Float64,1}}}\n│   %11 = Core.apply_type(%6, %7, %8, %9, %10)::Type{getfield(Main, Symbol(\"#δE#10\")){Int64,Float64,Float64,_A}} where _A\n│   %12 = hbar::Core.Compiler.Const(0.15915494309189535, false)::Core.Compiler.Const(0.15915494309189535, false)\n│         (δE = %new(%11, ω, t, %12, es))\n│   %14 = Main.:(##9#11)::Core.Compiler.Const(getfield(Main, Symbol(\"##9#11\")), false)\n│   %15 = Core.typeof(δE)::Type{getfield(Main, Symbol(\"#δE#10\")){Int64,Float64,Float64,_A}} where _A\n│   %16 = Core.apply_type(%14, %15)::Type{getfield(Main, Symbol(\"##9#11\")){_A}} where _A\n│         (#9 = %new(%16, δE))\n│   %18 = #9::getfield(Main, Symbol(\"##9#11\")){_A} where _A\n│   %19 = (1:n)::Core.Compiler.PartialStruct(UnitRange{Int64}, Any[Core.Compiler.Const(1, false), Int64])\n│   %20 = (1:n)::Core.Compiler.PartialStruct(UnitRange{Int64}, Any[Core.Compiler.Const(1, false), Int64])\n│   %21 = Base.product(%19, %20)::Core.Compiler.PartialStruct(Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}}, Any[Core.Compiler.PartialStruct(Tuple{UnitRange{Int64},UnitRange{Int64}}, Any[Core.Compiler.PartialStruct(UnitRange{Int64}, Any[Core.Compiler.Const(1, false), Int64]), Core.Compiler.PartialStruct(UnitRange{Int64}, Any[Core.Compiler.Const(1, false), Int64])])])\n│   %22 = Base.Generator(%18, %21)::Base.Generator{Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}},_A} where _A\n│   %23 = Base.collect(%22)::Array{_A,2} where _A\n│   %24 = Main.T(bdc, bac)::Any\n│   %25 = Base.broadcasted(Main.:*, %23, %24)::Any\n│         (mat = Base.materialize(%25))\n│   %27 = mat::Any\n│   %28 = Base.broadcasted(Main.:*, 1, Main.im)::Core.Compiler.Const(Base.Broadcast.Broadcasted(*, (1, im)), false)\n│   %29 = Base.broadcasted(Main.:/, %27, %28)::Any\n│   %30 = Base.broadcasted(Main.:/, %29, hbar::Core.Compiler.Const(0.15915494309189535, false))::Any\n│   %31 = Base.broadcasted(Main.:/, %30, 2.0)::Any\n│   %32 = Base.materialize(%31)::Any\n└──       return %32\n\n\nIt looks a bit scary, but most of it is not that important to understand. What is important to understand is that red text = bad and that most of the critical information is in the header called “Variables”. You see that the problematic variables are #9 (which is a unnamed variable), es, δE, mat and then also the body (but that’s because of the failed inference that happened beforehand). For example, es::Union{Array{Complex{Float64},1}, Array{Float64,1}} means that the eigenvalues es can be either an array of complex or floating point numbers, which is not surprising, because the function eigvals can realistically produce either as a valid output. The next problematic part is #9::getfield(Main, Symbol(\"##9#11\")){_A} where _A, which is (I think) related to the transition matrix function call T(bdc, bac) where it cannot determine what type the output is. Because of these two, it fails the inference for the output type of the inline function δE(i, j) = ((es[i] - es[j]) / hbar - ω) * t and the output matrix mat, which screws up the whole body of the function call.\nTo solve this, we can pre-calculate certain variables and pass them in as arguments to Hij, which will kill two birds with one stone. The first is the time spent doing useless calculation: The energy eigenvalues of \\(\\hat{H}_0\\) don’t change because \\(\\hat{H}_0\\) doesn’t depend on time and neither does the transition matrix, which depends only on the polarization of the EM wave. And yet, they take the most computational time, because you need to do eigendecomposition of \\(\\hat{H}_0\\) to get them, along with a bunch of computationally expensive matrix-matrix products. The other stoned bird is type instability where, by passing the eigenvalues and transition matrix as arguments, you make the compiler dispatch on their types and there’s no need for type inference within the function Hij anymore.\n\nfunction Hij_fast(tij, es, ω, t, bdc, bac)\n    hbar = 1/(2*π)\n    \n    n = length(es)\n    δE(i, j) = ((es[i] - es[j]) / hbar - ω) * t\n    mat = [exp(-1im * δE(j, i)) + exp(1im * δE(i, j)) for i in 1:n, j in 1:n] .* tij\n    @. mat / 1im / hbar / 2.0\nend;\n\n\ntij = T(4, [0.1; 0; 0])\nes = eigvals(H0(4))\n@time Hij_fast(tij, es, 1000, 0.1, 4, [0.1; 0; 0]);\n\n  0.000012 seconds (15 allocations: 3.906 KiB)\n\n\nMuch better. With the problematic parts pre-calculated first, it takes only ~12 us and 15 allocations to produce the operator Hij. The type instability is gone too:\n\n@code_warntype Hij_fast(tij, es, 1000, 0.1, 4, [0.1; 0; 0])\n\nVariables\n  #self#::Core.Compiler.Const(Hij_fast, false)\n  tij::Array{Complex{Float64},2}\n  es::Array{Float64,1}\n  ω::Int64\n  t::Float64\n  bdc::Int64\n  bac::Array{Float64,1}\n  #12::getfield(Main, Symbol(\"##12#14\")){getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}}\n  hbar::Float64\n  n::Int64\n  δE::getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}\n  mat::Array{Complex{Float64},2}\n\nBody::Array{Complex{Float64},2}\n1 ─ %1  = (2 * Main.π)::Core.Compiler.Const(6.283185307179586, false)\n│         (hbar = 1 / %1)\n│         (n = Main.length(es))\n│   %4  = Main.:(#δE#13)::Core.Compiler.Const(getfield(Main, Symbol(\"#δE#13\")), false)\n│   %5  = Core.typeof(es)::Core.Compiler.Const(Array{Float64,1}, false)\n│   %6  = Core.typeof(ω)::Core.Compiler.Const(Int64, false)\n│   %7  = Core.typeof(t)::Core.Compiler.Const(Float64, false)\n│   %8  = Core.typeof(hbar::Core.Compiler.Const(0.15915494309189535, false))::Core.Compiler.Const(Float64, false)\n│   %9  = Core.apply_type(%4, %5, %6, %7, %8)::Core.Compiler.Const(getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}, false)\n│         (δE = %new(%9, es, ω, t, hbar::Core.Compiler.Const(0.15915494309189535, false)))\n│   %11 = Main.:(##12#14)::Core.Compiler.Const(getfield(Main, Symbol(\"##12#14\")), false)\n│   %12 = Core.typeof(δE::Core.Compiler.PartialStruct(getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}, Any[Array{Float64,1}, Int64, Float64, Core.Compiler.Const(0.15915494309189535, false)]))::Core.Compiler.Const(getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}, false)\n│   %13 = Core.apply_type(%11, %12)::Core.Compiler.Const(getfield(Main, Symbol(\"##12#14\")){getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}}, false)\n│         (#12 = %new(%13, δE::Core.Compiler.PartialStruct(getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}, Any[Array{Float64,1}, Int64, Float64, Core.Compiler.Const(0.15915494309189535, false)])))\n│   %15 = #12::Core.Compiler.PartialStruct(getfield(Main, Symbol(\"##12#14\")){getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}}, Any[Core.Compiler.PartialStruct(getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}, Any[Array{Float64,1}, Int64, Float64, Core.Compiler.Const(0.15915494309189535, false)])])::Core.Compiler.PartialStruct(getfield(Main, Symbol(\"##12#14\")){getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}}, Any[Core.Compiler.PartialStruct(getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}, Any[Array{Float64,1}, Int64, Float64, Core.Compiler.Const(0.15915494309189535, false)])])\n│   %16 = (1:n)::Core.Compiler.PartialStruct(UnitRange{Int64}, Any[Core.Compiler.Const(1, false), Int64])\n│   %17 = (1:n)::Core.Compiler.PartialStruct(UnitRange{Int64}, Any[Core.Compiler.Const(1, false), Int64])\n│   %18 = Base.product(%16, %17)::Core.Compiler.PartialStruct(Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}}, Any[Core.Compiler.PartialStruct(Tuple{UnitRange{Int64},UnitRange{Int64}}, Any[Core.Compiler.PartialStruct(UnitRange{Int64}, Any[Core.Compiler.Const(1, false), Int64]), Core.Compiler.PartialStruct(UnitRange{Int64}, Any[Core.Compiler.Const(1, false), Int64])])])\n│   %19 = Base.Generator(%15, %18)::Core.Compiler.PartialStruct(Base.Generator{Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}},getfield(Main, Symbol(\"##12#14\")){getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}}}, Any[Core.Compiler.PartialStruct(getfield(Main, Symbol(\"##12#14\")){getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}}, Any[Core.Compiler.PartialStruct(getfield(Main, Symbol(\"#δE#13\")){Array{Float64,1},Int64,Float64,Float64}, Any[Array{Float64,1}, Int64, Float64, Core.Compiler.Const(0.15915494309189535, false)])]), Core.Compiler.PartialStruct(Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}}, Any[Core.Compiler.PartialStruct(Tuple{UnitRange{Int64},UnitRange{Int64}}, Any[Core.Compiler.PartialStruct(UnitRange{Int64}, Any[Core.Compiler.Const(1, false), Int64]), Core.Compiler.PartialStruct(UnitRange{Int64}, Any[Core.Compiler.Const(1, false), Int64])])])])\n│   %20 = Base.collect(%19)::Array{Complex{Float64},2}\n│   %21 = Base.broadcasted(Main.:*, %20, tij)::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(*),Tuple{Array{Complex{Float64},2},Array{Complex{Float64},2}}}\n│         (mat = Base.materialize(%21))\n│   %23 = mat::Array{Complex{Float64},2}\n│   %24 = Base.broadcasted(Main.:*, 1, Main.im)::Core.Compiler.Const(Base.Broadcast.Broadcasted(*, (1, im)), false)\n│   %25 = Base.broadcasted(Main.:/, %23, %24)::Core.Compiler.PartialStruct(Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Array{Complex{Float64},2},Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{0},Nothing,typeof(*),Tuple{Int64,Complex{Bool}}}}}, Any[Core.Compiler.Const(/, false), Core.Compiler.PartialStruct(Tuple{Array{Complex{Float64},2},Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{0},Nothing,typeof(*),Tuple{Int64,Complex{Bool}}}}, Any[Array{Complex{Float64},2}, Core.Compiler.Const(Base.Broadcast.Broadcasted(*, (1, im)), false)]), Core.Compiler.Const(nothing, false)])\n│   %26 = Base.broadcasted(Main.:/, %25, hbar::Core.Compiler.Const(0.15915494309189535, false))::Core.Compiler.PartialStruct(Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Array{Complex{Float64},2},Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{0},Nothing,typeof(*),Tuple{Int64,Complex{Bool}}}}},Float64}}, Any[Core.Compiler.Const(/, false), Core.Compiler.PartialStruct(Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Array{Complex{Float64},2},Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{0},Nothing,typeof(*),Tuple{Int64,Complex{Bool}}}}},Float64}, Any[Core.Compiler.PartialStruct(Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Array{Complex{Float64},2},Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{0},Nothing,typeof(*),Tuple{Int64,Complex{Bool}}}}}, Any[Core.Compiler.Const(/, false), Core.Compiler.PartialStruct(Tuple{Array{Complex{Float64},2},Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{0},Nothing,typeof(*),Tuple{Int64,Complex{Bool}}}}, Any[Array{Complex{Float64},2}, Core.Compiler.Const(Base.Broadcast.Broadcasted(*, (1, im)), false)]), Core.Compiler.Const(nothing, false)]), Core.Compiler.Const(0.15915494309189535, false)]), Core.Compiler.Const(nothing, false)])\n│   %27 = Base.broadcasted(Main.:/, %26, 2.0)::Core.Compiler.PartialStruct(Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Array{Complex{Float64},2},Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{0},Nothing,typeof(*),Tuple{Int64,Complex{Bool}}}}},Float64}},Float64}}, Any[Core.Compiler.Const(/, false), Core.Compiler.PartialStruct(Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Array{Complex{Float64},2},Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{0},Nothing,typeof(*),Tuple{Int64,Complex{Bool}}}}},Float64}},Float64}, Any[Core.Compiler.PartialStruct(Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Array{Complex{Float64},2},Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{0},Nothing,typeof(*),Tuple{Int64,Complex{Bool}}}}},Float64}}, Any[Core.Compiler.Const(/, false), Core.Compiler.PartialStruct(Tuple{Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Array{Complex{Float64},2},Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{0},Nothing,typeof(*),Tuple{Int64,Complex{Bool}}}}},Float64}, Any[Core.Compiler.PartialStruct(Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2},Nothing,typeof(/),Tuple{Array{Complex{Float64},2},Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{0},Nothing,typeof(*),Tuple{Int64,Complex{Bool}}}}}, Any[Core.Compiler.Const(/, false), Core.Compiler.PartialStruct(Tuple{Array{Complex{Float64},2},Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{0},Nothing,typeof(*),Tuple{Int64,Complex{Bool}}}}, Any[Array{Complex{Float64},2}, Core.Compiler.Const(Base.Broadcast.Broadcasted(*, (1, im)), false)]), Core.Compiler.Const(nothing, false)]), Core.Compiler.Const(0.15915494309189535, false)]), Core.Compiler.Const(nothing, false)]), Core.Compiler.Const(2.0, false)]), Core.Compiler.Const(nothing, false)])\n│   %28 = Base.materialize(%27)::Array{Complex{Float64},2}\n└──       return %28\n\n\nThere’s no red text anywhere and the compiler is able to infer all types at compile-time.\nNow we can get to solve the (set of) differential equations. Many of you probably did make your own Euler and Runge-Kutta solvers or had to deal with libraries like SUNDIALS, so you know how much of a pain can this be. Fortunately, Julia has a nifty package called DifferentialEquations.jl that makes this a very straightforward procedure. There’s a lot of bells and whistles and related libraries that you can play with, but in it’s most basic form, the library solves ODEs of the form \\(\\dot{u}(t) = f(u(t),t)\\).\nFirst you need to define your problem which is done by defining the right-hand-side of the ODE, either directly as f(u, p, t), or the in-place update f(du, u, p, t), where the output is stored in the mutable variable du (to save some allocations). The function f is in my case called schr_eq and as variables it takes the new time state \\(\\vec{\\psi}(t + dt)\\) in du, the current state \\(\\vec{\\psi}(t)\\) in u, all the parameters stored in an array p and the current time t. The whole thing gets packaged into a ODEProblem along with the initial condition psi0 and a tuple of initial and final time tspan and solving the differential system is as easy as calling solve(). In general, it attempts to do some heuristics to determine the optimal solving algorithm but, for some reason, it fails to do that properly with this specific problem (complex arrays throw it off) so I had to manually chose the Tsit5() algorithm (seems to be the default).\n\nfunction schr_eq(du, u, p, t)\n    ω, bz, bac, es, tij = p\n    du[:] = Hij_fast(tij, es, ω, t, bz, bac) * u\nend\n\nfunction solve_schr_eq(ω, tmax, bz, bac, psi0)\n    es = eigvals(H0(bz))\n    tij = T(bz, bac)\n    tspan = (0., tmax)\n    p = [ω, bz, bac, es, tij]\n    prob = ODEProblem(schr_eq, psi0, tspan, p)\n    solve(prob, Tsit5())\nend;\n\nTrying it out should be as easy as defining the initial state, e.g. \\(\\psi(0)=\\psi_1\\), set up some values of fields and pressing the button:\n\nBz = 3.22896 # Gauss\nBac = [0.1, 0, 0] # Gauss\ntmax = 10.0 # μs\npsi0 = [1.0+0im; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0]\nω = 1000\n\nsol = solve_schr_eq(ω, tmax, Bz, Bac, psi0);\n\nplot() # Blank plot canvas\nfor i in 1:8\n    plot!(t->abs(sol(t)[i])^2, 0, tmax, label=\"\\$\\\\psi_$i\\$\") # In-place update to add time trace of each state component to the plot\nend\nplot!(ylabel=L\"\\mathrm{\\left| \\psi \\right| ^2}\")\nplot!(xlabel=L\"\\mathrm{t [\\mu s]}\")\n\n\n\n\nIsn’t that exciting? No? Tough crowd.\nBut really, nothing happens and one might even think that we did something wrong. Why? For two reasons: First, this is quantum mechanics and you cannot just change into a state with arbitrary energy. If we feed the system with energy of 1 GHz and there’s no allowed state 1 GHz above the ground-state, it will just sit there and ignore what we do because it there’s nowhere to put the energy in. Second, there are symmetry considerations and not every state can freely change into another, even if we give it a kick with the correct amount of energy. This information is hidden in the transition matrix elements, where you can see that some of them are equal to zero, which means that the states don’t couple. E.g. for the linearly polarized light:\n\nT(1, [0.1; 0; 0])\n\n8×8 Array{Complex{Float64},2}:\n        0.0+0.0im  -0.0496596+0.0im  …        0.0+0.0im   0.121424+0.0im\n -0.0496596+0.0im         0.0+0.0im     0.0858776+0.0im        0.0+0.0im\n        0.0+0.0im   0.0496698+0.0im           0.0+0.0im        0.0+0.0im\n        0.0+0.0im         0.0+0.0im           0.0+0.0im        0.0+0.0im\n        0.0+0.0im  -0.0858248+0.0im           0.0+0.0im        0.0+0.0im\n -0.0495408+0.0im         0.0+0.0im  …  0.0856718+0.0im        0.0+0.0im\n        0.0+0.0im   0.0858776+0.0im           0.0+0.0im  0.0699364+0.0im\n   0.121424+0.0im         0.0+0.0im     0.0699364+0.0im        0.0+0.0im\n\n\nYou see that the ground-state doesn’t really couple to many others. You can try playing with this yourself and use different polarizations, like circular polarized light with \\(\\vec{B}_{ac} = B_x + iB_y\\) for example. However, there’s some coupling to the second excited state so let’s try to shine light that has frequency resonant with that transition:\n\nBz = 3.22896 # Gauss\nBac = [0.1; 0.0; 0.0] # Gauss\ntmax = 43.0 # μs\npsi0 = [1.0+0im; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0; 0.0]\nes = eigvals(H0(Bz))\nω = 2.0*π*abs(es[1] - es[2])\n\nsol = solve_schr_eq(ω, tmax, Bz, Bac, psi0);\n\nplot()\nfor i in 1:8\n    plot!(t->abs(sol(t)[i])^2, 0, tmax, label=\"\\$\\\\psi_$i\\$\")\nend\nplot!(ylabel=L\"\\mathrm{\\left| \\psi \\right| ^2}\")\nplot!(xlabel=L\"\\mathrm{t [\\mu s]}\")\n\n\n\n\nBeautiful, a good figure to end this post with. What’s happening here is that the atom is absorbing and emitting photons, moving between the ground and excited states, and the occupation of states \\(\\left| \\psi_i \\right| ^2\\) (probability to detect the electron in a given state) changes periodically in time as it does so. The states \\(\\psi_1\\) and \\(\\psi_3\\) are not directly coupled, but \\(\\psi_2\\) does couple to both and they are all close in energy, so the electron ends up sloshing between the three of them. Periodic occupancy changes between two states are called Rabi oscillations, which are used in EM cavities that are parts of many quantum information science experiments, for example this one (see this pop-sci article if you don’t understand the publication itself). We will probably get back to this sometime in the future, if I decide to do a part on the Jaynes-Cummings model.\n\nThat’s it for the first part of this series. We developed a basic method to solve static and dynamic quantum-mechanical problems and applied it to a simple real system. There are no hidden functions and all the dependencies can be downloaded directly in a running Julia session, so you need to download the packages as outlined at the beginning of this post and then you can just copy-paste the code as is and it will run. You can play around with it and try different physical parameters, different atoms (you’ll need to look up the coupling constants and nuclear/orbital spins configurations for a given atom) or try to rewrite it in your programming language of choice.\nFor the next part, I’m not yet sure if I’ll stick to spin systems, or if I’ll do something else and then come back to them in the future. But I have a couple of things in mind so, in the meantime, feel free to discuss and ask questions relating to anything that you’ve read here, I’m pretty sure that me or somebody else will be more than happy to answer the questions you have."
  },
  {
    "objectID": "posts/cmpm0/index.html",
    "href": "posts/cmpm0/index.html",
    "title": "Computational Physics for the Masses Part 0: Hello, World!",
    "section": "",
    "text": "This first (well, zeroth) entry will be with no code, simply because it’s the first one and deserves some introduction. The real first post is up along with this one, so if you don’t feel like reading my rambling about how the establishment is doing it all wrong, jump to the second section. If you don’t want to read about profound philosophy of representation of function spaces in computers either (really?), then just skip this part all-together and go straight to next part."
  },
  {
    "objectID": "posts/cmpm0/index.html#whats-all-this-about",
    "href": "posts/cmpm0/index.html#whats-all-this-about",
    "title": "Computational Physics for the Masses Part 0: Hello, World!",
    "section": "What’s all this about?",
    "text": "What’s all this about?\nThe inspiration to this series comes from a course on topology in condensed matter, which I really like because it does not only talk about the physics, but actually does it and shows the results. It doesn’t just throw formulas at you, but uses them to show how the physical systems behave, at least on the theoretical level. But it’s not perfect. It focuses on, unsurprisingly, topology in condensed matter physics, which might not be something that interests everyone. And even if it is something that some of you would be interested in, the required amount of previous knowledge in condensed matter or quantum mechanics might be too high of a potential barrier to overcome. The second issue is that it obfuscates a large part of the computational algorithms and you have to do a lot of digging into the source code to find out how it works under the hood.\nYou could argue that there is a lot blogs, code repos and books out there to solve the inadequacies of an online topology course, and you would probably be right. If you know what you’re doing and know where to look or have the correct people around you to ask, you can build yourself from scratch to the point where you can comfortably do computational physics at or higher than the level of the course. But most people don’t fall into this category. The people who don’t know where and what to look for or who to ask (i.e. not higher undergrad or grad students working on computational topics) will go on the Internet and get flooded by hundreds of “I programmed a double-pendulum in Python” types of resources or resources that are too advanced for someone who doesn’t (plan to) do this for a living. It’s unfortunate that there’s this huge empty chasm between the absolute basics and science-grade code, which makes it hard to do anything cool.\nThis resource gap is where I want to find a niche and do something fun for me and useful for others. In this series, I want to show how to do computational physics that’s more than just drawing the trajectory of Earth around the Sun and slowly build up to some real, modern physics with no shortcuts.\nSo, what will you find in this series?\n\nMe bumbling around. I’m an experimental condensed matter physicist by trade but, back in college and early grad school, I was also working in the computational side of things. So you should expect things to mostly work, but they will not always be as efficient and the code might sometimes look ugly.\nA lot of computational quantum mechanics and statistical physics and maybe a tiny bit of particle physics. Things like simple astronomy and mechanics are covered by many other resources out there and I simply don’t know enough physics related to the more advanced stuff from these fields. Sorry, no computational general relativity or fluid dynamics (probably).\nCode that just works. Every single line of code will be shown and every function will be discussed. I don’t like hiding the computation in computational physics and I want you to see the process of going from textbook physics to a finished fancy figure that you could see in a published paper. Being able to see a worked example is a good way to overcome the learning curve and going from reading about exciting physics to doing it.\n\nAnd what will you not find in my posts?\n\nA textbook on basic physics. I know I said that I don’t want this to become a collection of advanced physics topics, but there’s a limit to how much I can fit into this format. Through the text, I will do my best to reference freely available resources wherever possible, but don’t expect to understand everything I do here without knowledge of the basics. That doesn’t mean you won’t get anything out of this if you’re not a physics student or a full-grown physicist. You can always reach out to Physics Stack Exchange or Reddit AskPhysics where me and many others will help you if you have questions or just read the material that I link and you should be able to follow what’s going on.\nThe same boring code and programming paradigms as everywhere else. If you were like me, your computational physics and numerics classes were taught in C, C++, Java or Python. Or Matlab (my condolences). We all had it hammered into our heads that object oriented programming is the One True Way and when you become a big boy scientist, C++ and Fortran will be the only acceptable languages. You will see that that’s not necessarily true anymore and I have to be the change I want to see in the world of scientific computing. If you still want to do this the conservative way, you easily can, because I will show all of the code and you will always have something to base your programs on.\n\nSo, who is this series for? Well, firstly me, I guess. I do this in my off time as a hobby. But if it were just that, I could just as easily keep all the code in private repo and not bother with all the text and derivations. It’s also for other people to read and learn. If you’re a physics student that wants to learn how to do computational physics, then you’re the target audience of this series. Similarly, if you’re a grown up physicist or material scientist that wants to see how the sausage is made, you should find this interesting. If you come from outside of physics and want to see and learn cool things, you absolutely can. Do expect, however, that some parts of my explanations might not be enough for you. Or, even worse, they might feel like they make sense to you but they actually don’t. Go to the sites I linked and ask if you don’t understand something. I’m not going to pretend that there are no stupid questions, but it’s always preferable to ask them rather just keeping a stupid question to yourself and not having an answer. We all were new to this at a certain point and no scientist will ever get mad at you or make fun of you not knowing something if you show genuine interest in a topic.\n\nNo snakes in this codebase\nIt shouldn’t surprise anyone that there will be a lot of code in a computational physics blog. And because there’s a lot of it, it’s worth discussing, at least for a little bit, what programming language I’ll be using and what are my preferred alternatives (and why Python isn’t any of them).\nAs I already mentioned, you will not find any of the workhorse languages like C++ or Fortran here. I sometimes have a weak moment and do small programs in Fortran but I find programming in C or C++ thoroughly unenjoyable and avoid it. At first, I was considering doing this series using OCaml because the balance of functional and imperative features is very pleasing and it has a NumPy/SciPy-like library specifically for scientific computing - the wonderful Owl library. I decided against following this route because the language has some rough edges and alien syntax that would distract from the main point. Although, I might do a bit here and there, where I’ll be showing how to do parallel algorithms on a GPU and that will be most definitely with Futhark, which is a ML-style language, just like OCaml. Another option I considered was the Rust programming language. It’s fast, has good multidimensional array library with associated linear algebra and the built-in tooling is infinitely better than C++. But it’s a compiled, strict and static language which makes the development of programs slow and the code way too verbose.\nIn the end, I settled for the Julia language. It’s a general purpose language with strong focus on scientific computing and numerics that’s trying to, at least seemingly, replace languages like R, Matlab and (heavily library-dependent) Python. And it’s being rather good at it: it’s very approachable, fast (with a few caveats) and getting it running is as easy as downloading a single app or setting up a free account on a Jupyter notebook server. It’s by no means as fun as coding in Scheme or Common Lisp, but it gets the job done. Be warned, though, that it’s not all happy-fun-times. Julia was really hyped a couple of years ago, early in it’s life cycle before the 1.0 release, which lead to a surprising explosion of libraries and community support. The dust has settled and the language has eventually matured and stabilized but at the expense of a bunch of breaking changes. Because of this, many of the resources you find on the web will just not work (at least 2/3 of the linear algebra tutorials and cookbooks tell you to use functions like eigs which no longer exist in the main linear algebra library) and some of the really interesting libraries just died before they could have been useful (e.g. seamless GPU support for arrays doesn’t work if you’re using OpenCL like me). C’est la vie.\nAfter all this, we can finally get to the real stuff…\n\n\nComputers and vector spaces or: How many bits does it take to store the concept of a sine function?\nIn an ideal world, this is where I would write a small book worth of concepts and derivations relevant to Hamiltonian mechanics and quantum mechanics. I will not do that. Not because I want to keep it secret from you but because it would be just too much to write (Griffiths has almost 470 pages just for introduction to quantum mechanics). Those of you who will read the following few paragraphs and feel completely lost, will have to either ask me or the Internet, or search for a book or pdf. There’s plenty of resources. And don’t lose hope, things will get a bit more down-to-earth once the programming part begins.\nBefore we get to the nitty-gritty of quantum mechanics, let’s have a bit of an informal discussion in context of classical mechanics. Let’s start with the concept of a state. A state is (as described by Wikipedia) a complete description of a system. It’s a roster of all the relevant quantities that describe an object. For example in orbital dynamics a state of a planet would be determined by giving the planet’s position and momentum. You can also have systems that have more than just one planet, in which case, the state is determined by enumerating all of the individual planet’s positions and momenta. In classical mechanics, all these properties are just well defined numbers.\nThe second pair of concepts is the phase space and the equations of motion. The phase space is a space of all the possible states of a system. If we take the example of a point mass in 1D (or a marble on a string, if you want), it’s phase space will be represented by a 2D plane with momentum as one coordinate and position as another. Equations of motion are equations that describe the time evolution of a system - given an initial state, they will tell you in what state the system will be in the future (physics is really just a posh form of clairvoyance). As the system evolves, it will draw out a trajectory in the phase space which, in case of a time-invariant system, will be contour of constant energy. These trajectories are solutions to the Hamilton’s equations:\n\\[ \\frac{d\\mathbf{p}}{dt} = -\\frac{\\partial\\mathcal{H}}{\\partial\\mathbf{q}} \\quad , \\quad \\frac{d\\mathbf{q}}{dt} = \\frac{\\partial\\mathcal{H}}{\\partial\\mathbf{p}}.\\]\n\\(\\mathcal{H}\\) is the famous classical Hamiltonian and \\(\\mathbf{q}\\)s and \\(\\mathbf{p}\\)s are generalized coordinates and corresponding momenta. Generalized, because the coordinates need not be just Cartesian coordinates - they can be the angles of a pendulum, displacements in a vibrating sheet or many other things.\nNow that we have all that, let’s ruin it by introducing quantum mechanics.\nThe concept of state still exists, somewhat. The state is now represented by a vector in vector space called a Hilbert space and this change has some interesting consequences. The relationship between a state and observables got subverted, where the state can no longer be defined by an enumeration of numbers corresponding to its canonical positions and momenta as in classical physics. Conceptually, momentum or position (or any other measurable quantity) are not just numbers anymore, but operators and the possible values of measurable quantities are the eigenvalues of these operators, i.e. the possible values \\(a\\) (numbers) of any physical quantity \\(A\\) of a system in state \\(\\psi\\) are \\(\\hat{A} \\psi = a\\psi\\).\nLastly, Newton’s equations of motion are not a thing anymore and the evolution of a system is governed by the Schrödinger equation:\n\\[i\\hbar \\frac{d}{dt} \\left| \\Psi(t) \\right> = \\hat{H} \\left| \\Psi(t)\\right>\\]\nor the time-independent form:\n\\[ \\hat{H} \\left| \\Psi \\right> = E \\left| \\Psi \\right>,\\]\nWith \\(\\hat{H}\\) the Hamiltonian operator and \\(E\\) it’s eigenvalues - the possible energies of the system.\nHere we introduced the bra-ket notation commonly seen in physics. As I mentioned before, the state \\(\\Psi\\) lives in a Hilbert space of wave functions and the angled braces notation is used for vectors in this space. Usually, a state \\(\\left| ... \\right>\\) (ket) is represented as a column vector and \\(\\left< ... \\right|\\) (bra) is it’s hermitian conjugate. An inner product of two vectors in this notations is written as \\(\\left< ... | ... \\right>\\) and the outer product as \\(\\left| ... \\right> \\left< ... \\right|\\).\nThe only problem is that Hilbert spaces and their vectors and operators are not something you can store in computer memory - it’s a fully abstract concept and my CPU doesn’t have registers for spherical harmonics or any other functions. How can we do simulations of such systems?\nIt’s easy, actually. When I tell you to make a program that calculates a position in real space, what will you do? It’s not like a CPU has registers for meters either (I’m betting that the CPU manufacturers are getting hard from the idea of region-locked hardware based on local preferred unit system). You’ll chose a basis which defines a good coordinate system and give me the position as a linear combination of the basis vectors - just numbers of how many of each there are between me and the position. What about operators? Well, consider what operators do: They transform vectors in the Hilbert space. Sounds suspiciously similar to something doesn’t it?\nMore formally, if we have some complete orthonormal basis \\(\\{\\phi _i \\}_i\\), with \\(\\left< \\phi _i | \\phi _j \\right> = \\delta _{ij}\\), an operator \\(\\hat{A}\\) can be represented as:\n\\[ \\hat{A} = \\mathbf{1} \\cdot \\hat{A} \\cdot \\mathbf{1} = \\left[ \\sum_i \\left| \\phi_i \\right> \\left< \\phi_i \\right| \\right] \\cdot \\hat{A} \\cdot \\left[ \\sum_j \\left| \\phi_j \\right> \\left< \\phi_j \\right| \\right] = \\sum_{ij} \\left<\\phi_i\\right| \\hat{A} \\left|\\phi_j\\right> \\left| \\phi_i \\right> \\left< \\phi_j \\right| = \\sum_{ij} A_{ij} \\left| \\phi_i \\right> \\left< \\phi_j \\right|,\\]\nWhere \\(A_{ij}\\) are elements of a complex matrix \\(\\mathbf{A}\\), i.e. something that we can store in a computer memory and have the CPU do calculations with. You can do the same thing with a vector in the Hilbert space:\n\\[ \\left| \\psi \\right> = \\mathbf{1} \\cdot \\left| \\psi \\right> = \\left[ \\sum_i \\left| \\phi_i \\right> \\left< \\phi_i \\right| \\right] \\cdot \\left| \\psi \\right> = \\sum_i \\left<\\phi_i | \\psi \\right> \\left| \\phi_i \\right> = \\sum_i \\psi_i \\left|\\phi_i\\right>,\\]\nwhere \\(\\psi_i\\) are components of a complex vector \\(\\mathbf{\\psi}\\). Just an array of complex numbers. Armed with this representation, we can finally write down computer-solvable problems, for example the time-independent Schrödinger equation:\n\\[ \\sum_j H_{ij} \\psi_j = E\\psi_i . \\]\nIt’s fascinating how dynamics of systems that might be fundamentally disconnected from the reality of our daily lives, described by mathematical objects which simply don’t exist in physical world can still be reduced to a simple algebraic problem solvable on a computer sitting on the desk in front of you. Unreasonable effectiveness of math indeed, Mr. Wigner.\n\nThat’s it for the this part. If you feel let down by reading the name of this series and then just seeing a bunch of text and equations with no real computation don’t worry. Other parts of the series won’t be like this. The next part is already out, as I wrote both of them back-to-back, but I would recommend to take a break. Stare at the last three equations. There is an important message being conveyed in the last couple of paragraphs and it would be a shame if it got lost before moving on to the number crunching."
  },
  {
    "objectID": "posts/facelift/index.html",
    "href": "posts/facelift/index.html",
    "title": "Facelift",
    "section": "",
    "text": "New year, new me website look!\nSince its inception, this blog has been made using Pelican as the static site generator of choice. It became my go-to because it had the wonderful option to render Jupyter notebooks without me having to use any external tools, which saved me a lot of headaches. Unfortunately, the plugin that enabled this feature got abandoned and my personal computer got upgraded to one with Apple’s ARM processor, so I couldn’t revert to older Python versions where things worked smoothly. I somehow made it work for my most recent post, but it took a lot of finagling and I realized that this is not sustainable. Since then, I discovered Quarto, a technical publishing system that has the option to generate static websites as well. It works seamlessly with Jupyter notebooks, has a more streamlined workflow and I can see myself using it for other things, work-related or not, so I made the switch. I think all my older posts render correctly, so there’s no need to worry if you’re new or a returning reader."
  },
  {
    "objectID": "posts/cmpm2/index.html",
    "href": "posts/cmpm2/index.html",
    "title": "Computational Physics for the Masses Part 2: Back to Real Space",
    "section": "",
    "text": "In the last part, we have demonstrated the most basic tools of solving quantum-mechanical problems. In the end, it wasn’t that hard. We chose systems that had only a finite number of possible states, which naturally led to convenient matrix representation that fully encompassed all of the physics we needed. Today, we’ll be dealing with systems that, at least on surface, should be more familiar to most readers but suffer from some interesting problems on both, the abstract and practical level - dynamics of particles in real space.\n\n\nLosing momentum\nIt shouldn’t surprise anyone that, if we are talking real-space dynamics, we’ll start with momentum. What might be surprising, is that momentum is a rather involved concept. Everyone knows the simple relation we learned in elementary school, \\(\\vec{p} = m\\vec{v}\\), which relates momentum to the velocity. This is, however, a rather specialized case of a very general concept in physics. Take, for example, classical electromagnetic fields, which can’t really have mass and have multiple different velocities you can assign to them, yet they do have momentum.\nA perhaps better, or more general, way of defining what is momentum, is using the Newton’s laws of motion, where momentum is related to applied forces:\n\\[ \\vec{F} = \\frac{d\\vec{p}}{dt}. \\]\nFrom which \\(\\Delta\\vec{p} = \\int \\vec{F} dt\\). But that’s still rather unwieldy and there’s no guarantee that we know the forces at play (not to mention the fact that you usually don’t have any forces in quantum mechanics). The “real” big-boy definition of generalized momentum comes from analytical mechanics. If you have a Lagrangian \\(\\mathcal{L}\\) (a weird abstract-mathematical thing that defines action \\(S=\\int\\mathcal{L}dt\\) - for our purposes just a difference of kinetic and potential energy \\(\\mathcal{L}=T-U\\)), the generalized momentum is the functional derivative of the Lagrangian with respect to time-derivative of a generalized coordinate\n\\[ p_j=\\frac{\\partial\\mathcal{L}}{\\partial \\dot{q}_j}. \\]\nBut, because we work with Hamiltonian mechanics, you want to do a Legendre transform to conjure the Hamiltonian\n\\[ \\mathcal{H}(\\vec{q}, \\vec{p}, t) = \\vec{p}\\cdot\\dot{\\vec{q}} - \\mathcal{L}(\\vec{q}, \\dot{\\vec{q}}, t), \\]\nand there the canonical momentum is the solution to one of the Hamilton’s equations:\n\\[ -\\dot{p}_i = \\frac{\\partial\\mathcal{H}}{\\partial q_i}. \\]\nIf this looks too abstract and confusing to you, then I achieved my goal in demonstrating that momentum is not just mass multiplied by some vector called \\(\\vec{v}\\), which tells you how far will you move along a line in some amount of time. It can be like that, but there are situations (e.g. essentially all of quantum mechanics), where tunnel-visioning on just the naive image of marbles flying around will make it hard to gain intuitive understanding of the problem.\nSo, what is momentum in quantum mechanics? By Noether’s theorem, it’s the stuff that is conserved in a translation-invariant system. So if we have a Hamiltonian that looks the same when we move it by arbitrary distance, you can define a conserved quantity of stuff that we call momentum. After some hand-waving, you’d find out that the operator corresponding to this conserved stuff is\n\\[ \\hat{p}_i = -i\\hbar\\frac{d}{dx_i}. \\]\nWell OK, that doesn’t sound that bad. Can we find a basis in which we can easily work with it? Yes, we can - by solving this eigenproblem (in 1D, for simplicity):\n\\[ \\hat{p} \\; \\psi(x) = -i\\hbar\\frac{d}{dx}\\psi(x) = p \\psi(x). \\]\nThis is one of the simplest possible differential equations to solve, so without too much beating around the bush, the solution is\n\\[ \\psi(x) = A\\mathrm{e}^{\\frac{i}{\\hbar}p\\cdot x}. \\]\nAnd this is where it becomes problematic. The smaller issue is with normalization: The integral \\(\\int _{-\\infty}^{\\infty}dx \\; \\mathrm{e}^{ip\\cdot x}\\) doesn’t have a finite value, so it’s not possible to to define the constant prefactor \\(A\\) using the typical methods. What you can do, is use something called the delta function normalization. The idea there is to use the fact that the basis has to be orthonormal, i.e. \\(\\int dx \\; \\phi^*_n(x)\\phi_m(x) = \\delta_{n,m}\\), which then allows you to abuse the properties of Fourier transforms to arrive at a finite value.\nThe much more problematic part is the cardinality of the (infinite) set of possible momenta. The basis spans a space that’s much larger than any Hilbert space of any square-integrable functions. To solve both of this problems, we’ll limit ourselves to a finite volume of space and a finite resolution. Before we get to that, we should talk a little bit about the consequences of doing it.\n\n\nFeeling incomplete\nIn the previous part, we had systems that had a finite number of possible states (at most eight, in our examples). This meant that we could easily contain all the information about the system in our computer memory. But what happens if a system has too many states, potentially an infinite amount? Well, in that case, you have to survive with only a smaller subset of the basis. This is completely fine in most cases, as we are usually interested in the properties of the ground state or few low-lying states above it - this is where all the interesting quantum mechanics happens. Formally, this means that there exists a projector operator that selects out only the part we care about\n\\[ \\hat{P} = \\sum_{i<\\infty} \\left| \\phi_i \\right> \\left< \\phi_i \\right|, \\]\nthat has a complement operator \\(\\hat{Q} = \\hat{1} - \\hat{P}\\) which would project onto the ignored part of the basis. Any operator \\(\\hat{A}\\) then becomes\n\\[ \\hat{A} = \\hat{1} \\cdot \\hat{A} \\cdot \\hat{1} =\n\\left( \\hat{P} + \\hat{Q} \\right) \\cdot \\hat{A} \\cdot \\left( \\hat{P} + \\hat{Q} \\right) =\n\\hat{P} \\cdot \\hat{A} \\cdot \\hat{P} + \\hat{P} \\cdot \\hat{A} \\cdot \\hat{Q} + \\hat{Q} \\cdot \\hat{A} \\cdot \\hat{P} + \\hat{Q} \\cdot \\hat{A} \\cdot \\hat{Q} = \\\\\n\\sum_{ij} A_{ij} \\left| \\phi_i \\right> \\left< \\phi_j \\right| +\n\\left[ \\hat{P} \\cdot \\hat{A} \\cdot \\hat{Q} + \\hat{Q} \\cdot \\hat{A} \\cdot \\hat{P} + \\hat{Q} \\cdot \\hat{A} \\cdot \\hat{Q} \\right], \\]\nwhere \\(A_{ij}\\) is the computer memory representation of the operator. The terms in the square parens represent coupling to the high-energy subspace and the operator acting on the high-energy subspace itself, and they all get ignored because life of a operator sometimes sucks. You can do the similar trick with representation of states:\n\\[ \\left| \\psi \\right> = \\hat{1} \\left| \\psi \\right> =\n\\left( \\hat{P} + \\hat{Q} \\right) \\left| \\psi \\right> =\n\\sum_i \\psi_i \\left| \\phi_i \\right> + \\hat{Q} \\left| \\psi \\right>, \\]\nwhere we trash the high-energy \\(\\hat{Q} \\left| \\psi \\right>\\) term. In the end, we didn’t change anything in the way of doing calculations but you should keep this in mind. We’re no longer working with the full information about our system.\n\n\nWell, well, well…\nMost of you probably already guessed the systems we’ll be dealing with today when I said that we’ll do stuff that’s contained into a finite volume. Yes, it’s the everyone’s favorite particle in a infinite potential well - with some twist, of course.\nFormally, the Hamiltonian consists of a kinetic energy and potential terms. The kinetic energy is (in 1D)\n\\[ \\hat{T} = -\\frac{\\hbar^2}{2m} \\int_{-\\infty}^{\\infty}dx \\; \\left| x \\right> \\frac{d^2}{dx^2} \\left< x \\right|, \\]\nwhere \\(\\left| x \\right>\\) is the position basis wavefunction. The full position basis consists of the “bad” Dirac basis functions (i.e. the full set is \\(\\{ \\left| x \\right> \\}_{x \\in \\mathbb{R}}\\)) and the position operator acts as \\(\\hat{x} \\left| x \\right> = x \\left| x \\right>\\). The potential energy is formally expressed as\n\\[ \\hat{V} = \\int_{-\\infty}^{\\infty}dx \\; \\left| x \\right> V(x) \\left< x \\right|, \\]\nwith \\(V(x)\\) being a potential that includes both the infinite “walls” of the well and any additional potential \\(W(x)\\) inside:\n\\[ V(x) = \\begin{cases} \\infty & \\text{for} \\; x \\le 0 \\\\\nW(x) & \\text{for} \\; 0 < x < l \\\\\n\\infty & \\text{for} \\; x \\ge l \\end{cases} \\]\nwhere \\(l\\) is the width of our potential well. Any single-particle state in this representation can be written as\n\\[ \\left| \\psi \\right> = \\int _{-\\infty}^{\\infty}dx \\; \\psi(x) \\left| x \\right>, \\]\nwith \\(\\psi(x) = \\left< x | \\psi \\right>\\) as the wavefunction. As I said, the Dirac set \\(\\{ \\left| x \\right> \\}_{x \\in \\mathbb{R}}\\) is not good because the uncountably many basis functions are too densely spaced and cause too many singularities in calculations, so we need something else for numerics.\nWe’ll get to doing code soon, so I’ll get the used packages out of the way now:\n\nusing SparseArrays\nusing LinearAlgebra\nusing LinearMaps\nusing IterativeSolvers\nusing FFTW\nusing KrylovKit\nusing Plots\n\nLinearAlgebra and Plots are nothing new. FFTW is the wrapper for the popular FFTW library which will be used to do some representation transformations using Fourier transforms (sine transforms, really). The more fun ones are LinearMaps, IterativeSolvers and KrylovKit and they probably deserve some comments.\nFor reasons that you’ll see in a minute, we’ll be dealing with large matrices today. Most of them sparse, so we could, in principle, use the built-in sparse arrays for efficient matrix operations. But there’s a more interesting way of doing things.\nWhen we get down to brass tacks, the only thing we’re really doing is solving an eigenvalue problem, which has a relatively straight-forward implementation for small dense matrices and vectors. For bigger problems, the naive implementations (typically some forms of the QR algorithm) are not efficient or even feasible and you have to do some form of matrix-free algorithms. The nice thing about these methods is that you don’t need to know your matrix, only how it acts on a vector. A large class of these methods are Krylov subspace iterative methods, like those implemented in the venerable ARPACK library (used in many pieces of scientific software, including Python’s SciPy) or in the KrylovKit package we’ll use today, although not for eigendecomsposition. The other popular method is using the LOBPCG algorithm implemented in IterativeSolvers, which does a steepest descent to converge to the an extremal eigenvalue. At their core, all that both of the algorithms do is repeated matrix multiplication and normalization of a (initially random) vector, until it converges towards an eigenvector with the smallest or largest eigenvalue. By far the most computationally expensive part is the matrix-vector multiplication, so that part is left to the users, who have to supply a function that returns the vector after multiplication themselves. In lower level (or just plainly old) languages this is handled by rather clunky (but efficient) callbacks and in newer languages with first-class functions (as is the case in our situation) by simply passing a function that acts like matrix-vector multiplication. And this is where LinearMaps comes into play: It leverages a bunch of Julia language features that allow for efficient composition and operations on vector-to-vector linear maps (or operators) - in simpler terms, it allows you to construct something that behaves like a matrix just by telling the program what it does to a vector. This reduces the memory footprint and number of calculations, and will allow us to write some really nice code.\nAs usual, we need computational bases. Yes, plural. Previously, we had just one basis to represent everything, but this time, some things will be much more convenient to write down in different representations, so we’ll have two and transform between them as needed.\n\n\n(Not really) momentum representation\nThe first basis will be one where the kinetic energy operator \\(\\hat{T}\\) is diagonal, i.e. it consists of solutions to the equation (with \\(\\hbar\\) = 1)\n\\[ -\\frac{1}{2m} \\frac{d}{dx^2} \\phi_n(x) = E \\; \\phi_n(x) .\\]\nIn general, the solution is a linear combination of complex exponentials, but with the added boundary condition \\(\\phi_n(0) = \\phi_n(l) = 0\\) (the particle is confined to our potential well), it becomes:\n\\[ \\left< x | \\phi_n \\right> = \\phi_n(x) = \\sqrt{\\frac{2}{l}} \\sin{\\frac{n \\pi x}{l}} .\\]\n\nfunction ϕ(x, n, l)\n    sqrt(2/l) * sin.(n * π * x / l)\nend;\n\nHopefully, you all know how sine functions look like, but just in case, here are the first five of the basis functions.\n\nplot([x -> ϕ(x, n, 1) for n in 1:5], 0:0.001:1, label = [\"n = $n\" for _ in 1:1, n in 1:5])\nplot!(xlabel=\"\\$ x/l \\$\")\nplot!(ylabel=\"\\$ \\\\phi _n \\\\sqrt{l} \\$\")\n\n\n\n\nAs I said, we chose this basis so that the kinetic energy operator is diagonal, with \\(\\hat{T} \\left| \\phi_n \\right> = \\frac{n^2 \\pi ^2}{2 m l^2} \\left| \\phi_n \\right>\\), so the operator just multiplies the the vector by the energy eigenvalues:\n\nfunction T(n_max, mass, l)\n    f(v) = map(enumerate(v)) do (n, el)\n        n^2 * π^2 / (2 * mass * l^2) * el\n    end\n    LinearMap(f, n_max, issymmetric=true)\nend;\n\nThe more nitpicky of you will probably object that this is not a “true” momentum basis, but a basis of the square of momentum \\(\\hat{p}^2\\) and you would be right. In a confined system such as the infinite potential well, the “true” momentum basis is bit of a touchy subject (tl;dr: there’s no way to make momentum eigenfunctions work with our boundary conditions), but you’ll often see people use the name also for the kinetic energy basis because it’s shorter to type and still has many of the relations to the position representation. That’s at least the reason why I’m doing it right now.\nThis is our first encounter with the LinearMap type. We supply it with the the vector-to-vector linear map as a function, the size of the vector and a boolean that tells it whether or not the would-be matrix is symmetric. The function f(v) in our case just takes the vector and multiplies the n-th element by the corresponding eigenvalue, which is done using the map(iterable) do x ... end construct. This is just syntactic sugar for map(x -> ..., iterable) that’s more readable if the mapped function is too long.\nThis is also the place where I can demonstrate the first of the many conveniences of LinearMaps. After creating the “matrix”, which is in reality just a function, I can apply it to a vector using multiplication as if it were a regular matrix. Like so:\n\nv = [1; 1; 1]\ntest = T(3, 1, 1)\ntest * v\n\n3-element Array{Float64,1}:\n  4.934802200544679\n 19.739208802178716\n 44.41321980490211 \n\n\nThose of you, who follow along in a different language, have two options: Either use sparse matrices and use built-in sparse linear algebra (i.e. users of Eigen, SciPy, etc.) or use just bare functions and corresponding matrix-free method libraries. Or you can just follow along with dense linear algebra, but don’t be surprised when numerics with 10000x10000 complex number matrices become slow or memory-heavy.\nYou might have noticed, that in the plot of basis functions, I said first five. That’s because there is no last basis function. The energy of the particles has (on our level of discussion) no upper limit and we need to introduce some cutoff n_max, beyond which all the states are unrepresentable. This has consequences that go beyond just maximum allowed energy of our system…\n\n\n(Not really) position representation\nAs I already mentioned in the intro, the position basis is not good for more than just practical reasons. The easiest way to go around it is to slap your system on a discrete grid, something that’s essentially forced on us by using the cutoff in energy. If we have a partial momentum basis \\(\\left\\{ \\left| \\phi_n \\right> \\right\\}_{n=1}^{n_{max}}\\), we can define an evenly-spaced grid with \\(n_{max}\\) points \\(\\left\\{ x_j \\right\\}_{j = 1}^{n_{max}} = \\left\\{ \\Delta \\cdot j \\right\\}_{j = 1}^{n_{max}}\\) with \\(\\Delta = \\frac{l}{n_{max} + 1}\\):\n\nfunction make_grid(n_max, l)\n    Δ = l / (n_max + 1)\n    (Δ, [i * Δ for i in 1:n_max])\nend;\n\nWith these, we can define a new basis of not-entirely-Dirac-delta functions:\n\\[ \\left| \\vartheta_j \\right> = \\sqrt{\\Delta} \\sum _{n = 1} ^{n_{max}} \\phi_n (x_j) \\left| n \\right> ,\\]\nand that can be used to write down the wave functions in real-space as\n\\[ \\left< x | \\vartheta_j \\right> = \\vartheta_j(x) = \\sqrt{\\Delta} \\sum _{n = 1} ^{n_{max}} \\phi_n(x_j)\\phi_n(x) .\\]\n\nfunction ϑ(x, j, l, n_max)\n    Δ = l / (n_max + 1)\n    sqrt(Δ) * sum([ϕ(Δ * j, n, l) * ϕ(x, n, l) for n in 1:n_max])\nend;\n\nThe real-space position basis functions corresponding to the 5 previous ones look likes this:\n\nplot([x -> ϑ(x, j, 1.0, 5) for j in 1:5], 0:0.001:1, label = [\"j = $j\" for _ in 1:1, j in 1:5])\nplot!(xlabel=\"\\$ x/l \\$\")\nplot!(ylabel=\"\\$ \\\\vartheta _j \\\\sqrt{l} \\$\")\n\n\n\n\nIt might be a bit hard to see what’s going on in that plot, so let’s select just one wave function close to the center with a higher cutoff \\(n_{max}\\) = 50:\n\nplot(x -> ϑ(x, 25, 1.0, 50), 0:0.001:1, label=\"\")\nplot!(xlabel=\"\\$ x/l \\$\")\nplot!(ylabel=\"\\$ \\\\vartheta _{25} \\\\sqrt{l} \\$\")\n\n\n\n\nThese basis functions are orthonormal and highly localized, in the sense that they that they have non-trivial values only close to \\(x_j\\) for \\(\\vartheta_j\\) and are close to zero everywhere else (they are actually zero at every point \\(x_{j\\prime}\\) for \\(j\\prime \\ne j\\)), or in math form:\n\\[ \\vartheta_j (x_{j\\prime}) = \\frac{\\delta_{j j\\prime}}{\\sqrt{\\Delta}} .\\]\nThis is super useful for a few reasons:\n\nIf we have a real-space wave function as a vector in the position basis \\(\\left| \\psi \\right> = \\sum _j \\psi_j \\left| \\vartheta _j \\right>\\), then we know it’s values at the grid points: \\[ \\psi (x_j) = \\left< x_j | \\psi \\right> = \\left< x_j \\right| \\sum _{j\\prime} \\psi_{j\\prime} \\left| \\vartheta _{j\\prime} \\right> = \\sum _{j\\prime} \\psi _{j\\prime} \\left< x_j | \\vartheta_{j\\prime} \\right> = \\frac{\\psi_j}{\\sqrt{\\Delta}} .\\] This means that, if the grid is fine enough, we can simply interpolate between grid points to get the wave function in the whole space.\nIf a function \\(f(x)\\) changes slowly over the interval \\(\\Delta\\) at \\(x_j\\), we can approximate it by values only at the grid points: \\[ f(x)\\vartheta_j(x) \\approx f(x_j)\\vartheta_j(x) .\\] We’ll (ab)use this fact a lot when defining, e.g., the potential energy operator.\n\nThese two points are also why I said that we’ll be dealing with large matrices today. All approximations become exact in the limits of \\(n, j \\to \\infty\\), so you want a dense grid for them to be good enough for calculations.\nWith this, we have all we need to define our potential operator, which is just a carbon copy of what I wrote in the second bullet point, just with \\(f(x) \\mapsto W(x)\\). The only thing the potential operator does (usually), is to multiply the wave function with the value of the potential at a given point which makes it very easy to write in the position basis:\n\nfunction V(n_max, Wg)\n    f(v) = v .* Wg\n    LinearMap(f, n_max; issymmetric=true)\nend;\n\nWith the operators \\(\\hat{T}\\) and \\(\\hat{V}\\), we have all we need to write down the Hamiltonian \\(\\hat{H} = \\hat{T} + \\hat{V}\\), but there’s a catch: The two operators are currently in different representations, so we can’t just add them as is. We need a way to switch between the momentum and position representation.\n\n\nTransformations\nOK, so how do we make the our current two operators compatible? We need to change representation of one of them into the representation used for the other. There’s nothing that should stop us in doing that, as the state doesn’t care in which basis we expand it in:\n\\[ \\left| \\psi \\right> = \\sum _{n = 1} ^{n_{max}} \\psi_n \\left| \\phi_n \\right> = \\sum _{j = 1} ^{n_{max}} \\psi_j \\left| \\vartheta _j \\right> .\\]\nThe coefficients \\(\\psi_n\\) and \\(\\psi_j\\) are of course different, but the state \\(\\left| \\psi \\right>\\) is still physically the same state. If you scroll up a bit and look at the definition of \\(\\left| \\vartheta_j \\right>\\), you see that we can insert it into the previous equation and have an expression with just \\(\\phi_n\\):\n\\[ \\sum _{n = 1} ^{n_{max}} \\psi_n \\left| \\phi_n \\right> = \\sum _{j = 1} ^{n_{max}} \\psi_j \\left[ \\sqrt{\\Delta} \\sum _{n\\prime = 1} ^{n_{max}} \\phi_{n\\prime} (x_j) \\left| \\phi_{n\\prime} \\right> \\right] = \\sum _{n\\prime = 1} ^{n_{max}} \\left[ \\sqrt{\\Delta} \\sum _{j = 1} ^{n_{max}} \\psi_j \\phi_{n\\prime} (x_j) \\right] \\left| \\phi_{n\\prime} \\right> .\\]\nAs the basis \\(\\left\\{ \\left| \\phi_n \\right> \\right\\}\\) is orthonormal, the above equation reduces to:\n\\[ \\psi_n = \\sqrt{\\Delta} \\sum _{j = 1} ^{n_{max}} \\psi_j \\phi_n (x_j) = \\sum _{j = 1} ^{n_{max}} X_{n j} \\psi_j ,\\]\nwhere we introduced the representation transform matrix:\n\\[ X_{n j} = \\left< \\phi_n | \\vartheta_j \\right> = \\sqrt{\\Delta} \\phi_n (x_j) = \\sqrt{\\frac{l}{n_{max} + 1}} \\sqrt{\\frac{2}{l}} \\sin{\\left( \\frac{n \\pi x_j}{l} \\right)} = \\sqrt{\\frac{2}{n_{max} + 1}} \\sin{\\left( \\frac{n j \\pi}{n_{max} + 1} \\right)} ,\\]\nwhich is about as convenient to use as it is to write down in LaTex (not at all). The good news is that the matrix is symmetric and works both ways, i.e. \\(\\psi_n = \\sum _{j = 1} ^{n_{max}} X_{n j} \\psi_j\\) and \\(\\psi_j = \\sum _{n = 1} ^{n_{max}} X_{n j} \\psi_n\\). We could just take this definition and use it define our transformation operator, but that wouldn’t be smart because it’s a lot of typing and (more importantly) it’s not efficient - this brute force approach requires \\(n_{max}^2\\) operations. If you look at the above equation for long enough, you’ll recognize that it’s a sine transform. Fortunately, discrete sine transforms (our case is a DST-I) can be calculated efficiently using Fourier transforms (with asymptotic complexity \\(\\mathcal{O}(n_{max}\\cdot\\log{\\left( n_{max} \\right) })\\) and the FFTW library has routines specifically for that purpose so we’ll use that:\n\nfunction Rpr(n_max)\n    norm = sqrt(2 * (n_max + 1))\n    LinearMap(v -> FFTW.r2r(v, FFTW.RODFT00) / norm, n_max; issymmetric=true)\nend;\n\nThe transform operator \\(\\hat{X}\\) is called Rpr in my code, because X is too common of a letter. The only peculiarity is that FFTW doesn’t normalize the transforms according to conventions used in most the literature and software, so we need to do that by hand. It can be directly applied to a state vector to switch from one representation to another and, if we want to use it to transform operators, we just have to do \\(\\hat{A}_M= \\hat{X} \\cdot \\hat{A}_P \\cdot \\hat{X}\\) if we want to switch from position to momentum or vice versa (think of it as the operators acting on a vector from right to left - transform the state, act with the operator, transform back).\n\n\nThe situation of gravity\nLet’s put all of it to use. We’ll do something that everyone should be familiar with from daily experience - gravitational field \\(W(x) = m g x\\). The “ground” in our case is at x = 0 and x > 0 is away from the ground, so “up” is to the right on all the plots that follow.\nWe’ll follow the old tradition of assuming no friction or air resistance, and make the spherical cow a quantum cow (and name it “Particle”). The Hamiltonian of our quantum cow is rather simply:\n\\[ \\hat{H} = -\\frac{\\hbar^2}{2 m} \\frac{d^2}{dx^2} + m g \\hat{x}. \\]\nOn top of defining the Hamiltonian, we’ll also furnish the problem with a boundary condition \\(\\psi(x) = 0\\) for \\(x \\le 0\\) because cows don’t live underground.\nNow is the time to talk about characteristic sizes of things - choice of units will be somewhat arbitrary in this case, as we’ll be doing everything in relation to these characteristic sizes and everything is dimensionless. The characteristic length will be \\(l^* = \\sqrt[3]{\\frac{\\hbar^2}{m^2 g}}\\), which is roughly the extent of the ground state of the above Hamiltonian. Using this, we can define the characteristic time \\(t^* = \\frac{m{l^*}^2}{\\hbar} = \\sqrt[3]{\\frac{\\hbar}{mg^2}}\\) and the characteristic energy \\(E^* = \\frac{\\hbar}{t^*} = \\sqrt[3]{m g^2 \\hbar^2}\\). When you try to express the physical constants using \\(E^*\\), \\(t^*\\) and \\(l^*\\), you’ll find out that \\(\\hbar = g =1\\) and everything in the Hamiltonian scales just with the mass \\(m\\), which we could also pin against some mass, and have it in multiples of, let’s say, the mass of an electron \\(m^* = \\frac{m}{m_e}\\) (this is why they teach you dimensional analysis in school, kids).\nThe problem above has an analytical solution in terms of Airy functions (named after George Airy, not because they are light) and you can search the Internet for how to do that. Triangular potential wells commonly appear in semiconducting electronics, when you make 2D electron gases or in the WKB approximation, so don’t panic if Google throws a bunch of electronics research papers on you. Airy functions are defined as solutions to the Airy differential equation, so the only thing you have to do is to convince yourself that the Schrödinger’s equation with the above Hamiltonian has the same form.\nI can already hear the “Less talking, more coding”, so here’s the function that constructs the above Hamiltonian:\n\nfunction HP(n_max, W, m, l)\n    Δ, xs = make_grid(n_max, l)\n    TP = Rpr(n_max) * T(n_max, m, l) * Rpr(n_max)\n    VP = V(n_max, W.(xs))\n    TP + VP\nend;\n\nNo surprising code here. We create the kinetic and potential energy operators using the function we defined a few paragraphs above. The kinetic energy operator created by the function T is in momentum representation, so we change it to operator in position representation TP by using the representation transform operator. Potential energy operator VP is in the correct representation, so we can add them both to get the Hamiltonian in position basis (you could do it the other way around, but I will not do that so that I can show a few things).\nOne last thing that requires a little bit of thought is the resolution and span of our model space, which we need to set to values that will allow us to get good precision and don’t take forever to calculate. The real physical system as shown above exists in a semi-infinite space (\\(x > 0\\)), which is obviously something we cannot replicate in the numerical calculation, that will be done in a box (\\(l > x > 0\\)). How big you want to make the box depends on how many states you want to recover. All the states are bound states and their wave functions will go to zero as \\(x\\) approaches infinity, however, they spread out as they get higher in energy. You want to make the box big enough for your highest energy state of interest to be almost fully contained within it (there will be always tail of the wave function outside of the box, but that should be close to zero). Next up is the resolution (or the size of our Hilbert space) defined by the cutoff. Obviously, the cutoff needs to be bigger than the number of states you’re interested in - you can’t represent the 11-th excited state, if your operators span only the subspace of the 5 states (unless you’re lucky or smart about your basis, but that’s besides the point). It also needs to be high enough to have the resolution to faithfully represent functions of the position (like the potential). A bit trial and error shows that \\(l = 10 \\, l^*\\) and \\(n_{max} = 30\\) is good enough for our first attempt.\nFirst, we should confirm that it works. The analytical ground-state energy is \\(E_1 \\approx 1.8558 \\, E^*\\), so let’s solve the eigenproblem and compare (with \\(m = 1\\), so I simply used the identity function for potential):\n\nH = HP(30, identity, 1, 10)\nr = lobpcg(H, false, 3)\nr.λ[1]\n\n1.855948262589037\n\n\nThe ground-state energy is close to analytical value so, by induction from this base case, I claim that it works everywhere (using this proof in an academic setting is not recommended). The call to the lobpcg eigensolver is pretty straightforward. You supply it with the linear operator you want to decompose, a boolean which tells it if you want the largest (true) or smallest (false) eigenvalues and a integer for how many eigenvalues you want. It returns a object that contains a bunch of info about the algorithm and convergence and also a vector of eigenvalues λ and a matrix X which holds the corresponding eigenvectors as columns.\nLet’s see how the probability densities look in real space:\n\nΔ, xs = make_grid(30, 10)\nes, evs = r.λ, r.X\nplot(xs .- 0.5*Δ, [abs2.(evs[:, i]) ./ Δ .+ real(es[i]) for i in 1:3], line=:steppre, label=\"\")\nplot!(identity, 0:5, line=:dash, label=\"\")\nxlims!(0,6)\nplot!(xlabel=\"\\$ x/l^* \\$\")\nplot!(ylabel=\"\\$ \\\\left| \\\\psi \\\\right|^2 \\$\")\n\n\n\n\nThe probability densities are offset from zero by their energies, so you can see the well-known quantum effect of the particle tunneling outside of the potential well (the gravitational potential is depicted by the dashed line).\nI purposefully got rid of the default linear interpolation and kept the wave function constant within the interval \\(\\Delta\\) around each grid point to demonstrate how bad the real-space resolution is. The obvious solution is to simply increase \\(n_{max}\\), but keep in mind that the number of matrix elements scales as \\(n_{max}^2\\), so even doubling the number would go from 900 to 3600 elements. By standards of computational algebra, those are still tiny numbers but the quadratic scaling will hurt eventually. Fortunately, we have another option: As long as the Hilbert sub-space is large enough to represent the state with good enough fidelity (which it supposedly does, judging by the eigenvalues) we can change into the momentum basis and use the fact the the functions \\(\\phi_n(x)\\) are continuous and defined in the whole range \\(0 < x < l\\). Currently, our states are vectors with components in the position basis \\(\\vec{\\psi} = \\psi_j\\), but we can use the representation transform operator to get the components in momentum basis \\(\\psi_n = \\sum _{j = 1} ^{n_{max}} X_{n j} \\psi_j\\). The wave function will then be reconstructed as a sum over the momentum basis functions:\n\\[ \\psi(x) = \\sum_{n = 1} ^{n_{max}} \\psi_n \\phi_n(x) = \\sum_{n = 1} ^{n_{max}} \\sum _{j = 1} ^{n_{max}} \\phi_n(x) X_{n j} \\psi_j,\\]\nlike this:\n\nψ(x, i) = [ϕ(x, n, 10) for n in 1:30]' * (Rpr(30) * evs[:, i])\n\nplot([x -> abs2.(ψ(x, i)) + real(es[i]) for i in 1:3], 0:0.05:10, label=\"\")\nplot!(identity, 0:5, line=:dash, label=\"\")\nxlims!(0,6)\nplot!(xlabel=\"\\$ x/l^* \\$\")\nplot!(ylabel=\"\\$ \\\\left| \\\\psi \\\\right|^2 \\$\")\n\n\n\n\nThis makes the plot much more pleasing to the eye and better shows the probability leaking outside of the classically allowed region. It also makes it much easier to recognize the the the wave functions really are the Airy functions.\n\n\nMove it!\nTo spice things up a little bit, let’s do some dynamics. In the previous part of this series, I went into the ins and outs of solving the time-dependent Schrödinger equation. In the final example, we coded a solver for the differential equation that gave us an approximate numerical solution because there was no way to solve it analytically and, in principle, we could do the same thing here. There is, however, a different way this time - remember, that you can get a simple expression for the time evolution operator if the Hamiltonian commutes with itself at different times, which allows me to write the time-dependent solution straight away:\n\\[ \\left| \\psi(t) \\right> = e^{-\\frac{i}{\\hbar}\\int _{t_0} ^{t} d\\tau \\hat{H}(\\tau)} \\left| \\psi(t_0) \\right> = e^{-\\frac{i (t - t_0)}{\\hbar}\\hat{H}} \\left| \\psi(t_0) \\right> ,\\]\nwhere the last part uses the fact that our Hamiltonian doesn’t depend on time at all. The good news is that getting a solution is as easy as taking some initial state vector and act on it with the exponential of the Hamiltonian multiplied by some scalar. The bad news is that matrix exponentials are difficult to do numerically as can be seen even from the Wikipedia article. The obvious choice is do some form of series expansion, which is the go-to method for people doing analytical work. Many popular pieces of scientific computing software (like Octave or SciPy if Wikipedia is to be believed) use the Padé approximant but that’s not good for large matrices because you have to invert a large matrix (of the same size as the exponentiated one) at a certain step of the calculation. A much better way is to use the fact that the exponential of a diagonal matrix is the same as a matrix with exponentials of the diagonal elements - to exponentiate a diagonal matrix you just apply the exponential element-wise. Of course, this saves you time only if you can diagonalize the matrix efficiently, but that’s not an issue for us because our operator matrices are diagonal by construction…in their respective basis. The only thing that stands between us and a simple solution is the addition of two operators in the Hamiltonian which are not diagonal at the same time (doesn’t mean that there is no basis in which they can be simultaneously diagonalized, we just don’t have it). Fortunately, there is something called the Baker-Campbell-Hausdorff formula (related to the Lie-Trotter product) that leads to a formula that’s very reminiscent of Verlet (or second order sympletic) splitting method:\n\\[\\left| \\psi(t) \\right> = e^{-\\frac{i (t - t_0)}{\\hbar} \\left( \\hat{T} + \\hat{V} \\right)} \\left| \\psi(t_0) \\right> \\approx \\left[ e^{-\\frac{i(t-t_0)}{2 N \\hbar}\\hat{V}} \\cdot e^{-\\frac{i(t-t_0)}{N \\hbar}\\hat{T}} \\cdot e^{-\\frac{i(t-t_0)}{2 N \\hbar}\\hat{V}} \\right]^N \\left| \\psi(t_0) \\right> ,\\]\nwhich holds for large \\(N\\). Here, it’s important that the exponential of sum of operators changed into a product of operator exponentials, which allows you to represent each of the exponentials in the basis where the operator is diagonal and do the representation transform after, i.e. calculate the \\(\\hat{T}_e = e^{-\\frac{i(t-t_0)}{N \\hbar}\\hat{T}}\\) in the momentum basis and then use \\(\\hat{X} \\cdot \\hat{T}_e \\cdot \\hat{X}\\) to bring it to position basis (assuming that the potential operator and the state are in position representation). The price you pay here is that you have to split the time interval \\(\\delta t = (t-t_0)\\) into \\(N\\) small steps, so you’ll be doing a lot of operator applications.\n\nfunction eV(n_max, N, δt, Wg)\n    α = -1im * (δt / N)\n    f(v) = v .* exp.(α .* Wg ./ 2)\n    LinearMap{ComplexF64}(f, n_max; issymmetric=true)\nend\n\nfunction eT(n_max, N, δt, mass, l)\n    α = -1im * (δt / N)\n    f(v) = map(enumerate(v)) do (n, el)\n        exp(α * n^2 * π^2 / (2 * mass * l^2)) * el\n    end\n    LinearMap{ComplexF64}(f, n_max; issymmetric=true)\nend\n\nfunction U_split(n_max, N, δt, W, mass, l)\n    Δ, xs = make_grid(n_max, l)\n    VP = eV(n_max, N, δt, W.(xs))\n    TP = Rpr(n_max) * eT(n_max, N, δt, mass, l) * Rpr(n_max)\n    VP * TP * VP\nend\n\nfunction split_evolve(n_max, W, δt, N, ψ0, mass, l)\n    t = 0.0\n    t_out = [t]\n    ψ_out = hcat(ψ0)\n    UP = U_split(n_max, N, δt, W, mass, l)\n    for n in 1:N\n        t += δt/N\n        append!(t_out, t)\n        ψ_out = hcat(ψ_out, UP * ψ_out[:, end])\n    end\n    (t_out, ψ_out)\nend;\n\nFunctions eV and eT are the operator exponentials in their preferred basis and the function U_split returns the time propagator in position representation. split_evolve handles the repeated applications of the approximate time evolution operator to do time propagation.\nAnother option is to use Krylov subspace of the matrix to find an optimal polynomial approximation to the exponential. The idea is that the exponential is dominated by only a small part of the full spectrum, so one can calculate the approximate action of exponentiated operator using a much smaller projection onto the appropriate subspace:\n\\[e^{tH_N} \\cdot \\vec{v} \\approx \\left| \\left| \\vec{v} \\right| \\right|_2 V_M \\cdot e^{tH_M} \\cdot \\vec{e}_1 ,\\]\nwhere \\(V_M = [\\vec{v}_1, \\ldots, \\vec{v}_M]\\) are the basis vectors \\(\\{\\vec{v}_i\\}\\) of an order-M Krylov subspace stacked into a matrix and \\(H_M = V_M ^T \\cdot H \\cdot V_M\\) is the image of the original matrix in the subspace (\\(\\vec{e}_i\\) is just a unit vector in the basis). The important thing is, that the Krylov subspace can be small (even for matrices with sizes of more than 1000x1000, it might still be sufficient to have \\(M < 50\\)) which makes it possible to calculate the exponential by the usual means. For those who are interested in implementing it, Expokit has the algorithm documented in full detail. We don’t need to do that, as the KrylovKit package already comes with this algorithm (the exponentiate function) which makes the functions much easier to write and allows us to propagate the state with arbitrary time-steps (and it uses the already defined Hamiltonian operator generated by the function H). The trade-off is that it is much slower when you want to do a fine-grained time trace, as each propagation takes a considerable amount of computation time because of the iteration needed to construct the Krylov subspace.\n\nfunction U_krylov(H, δt, ψ0)\n    dt = -δt * 1im\n    exponentiate(v -> H * v, dt, ψ0)\nend\n\nfunction krylov_evolve(H, δt, nsteps, ψ0)\n    t = 0.0\n    t_out = [t]\n    ψ_out = hcat(ψ0)\n    for n in 1:nsteps\n        t += δt\n        new, info = U_krylov(H, δt, ψ_out[:, end])\n        ψ_out = hcat(ψ_out, new)\n        append!(t_out, t)\n    end\n    (t_out, ψ_out)\nend;\n\nThe U_krylov function handles propagation from state ψ0 by a time-step δt under action of Hamiltonian H and the function krylov_evolve does nsteps of such evolutions.\nAs with every solution to an equation of motion, we have to supply some initial condition, which will be a Gaussian packet (a wave packet with minimum uncertainty in position and momentum allowed):\n\nfunction gaussian(x, μ, σ)\n    exp(-((x - μ) / (2 * σ))^2)\nend\n\nfunction krylov_particleInWell(n_max, tmax, tsteps, x0, m, l)\n    Δ, xs = make_grid(n_max, l)\n    δt = tmax / tsteps\n    \n    ψ0 = [gaussian(x, x0, 1) for x in xs]\n    ψ0 = ψ0 ./ norm(ψ0) # wave function should be properly normalized\n\n    H = HP(n_max, identity, m, l)\n    \n    krylov_evolve(H, δt, tsteps, ψ0)\nend\n\nfunction split_particleInWell(n_max, tmax, tsteps, x0, mass, l)\n    Δ, xs = make_grid(n_max, l)\n    \n    ψ0 = [gaussian(x, x0, 1) for x in xs]\n    ψ0 = ψ0 ./ norm(ψ0) # wave function should be properly normalized\n    \n    split_evolve(n_max, identity, tmax, tsteps, ψ0, mass, l)\nend;\n\nWe’ll put the particle to \\(x_0 = 15 \\, l^*\\) to do it’s thing for \\(10\\sqrt{2x_0}\\) time units (remember, time is in units of \\(t^*\\)) split into 400 steps in the same gravitational well as in the static example. We’ll need better resolution in this example, so I increased \\(n_{max}\\) to 1000 (this corresponds to 1 million matrix elements).\n\nx0 = 15\nτ = √(2*x0)\n\n# Try both algorithms and play with the values, to get a feeling for their situational use\n# ts, ψs = krylov_particleInWell(1000, 10*τ, 400, x0, 1, 30);\n\nts, ψs = split_particleInWell(1000, 10*τ, 400, x0, 1, 30);\n\nAfter the computation finishes (which should be quick if you’re using the Verlet solver), we can make a nice animation of the particle moving around in the potential well. For comparison, I also added a classical point mass so you can see the correspondence between the quantum and classical mechanics.\n\nΔ, xs = make_grid(1000, 30)\n\nfunction x_classical(t, x0, g)\n    t1 = sqrt(2*x0 / g)\n    r = (-t1):(-t1 + 2t1)\n    n = length(r)\n    x0 - 0.5*(mod(t - first(r), n) + first(r))^2\nend\n\nbouncing = @animate for (ψ, t) in zip(eachcol(ψs), ts)\n    plot(xs, abs2.(ψ) ./ Δ, fill = 0, label=\"\\$ \\\\left| \\\\Psi \\\\right| ^2\\$\", color=:darkred)\n    scatter!([x_classical(t, 15, 1)], [0.65], markersize=10, label=\"Classical\", color=:grey)\n    yticks!(:none)\n    xlims!(0,20)\n    ylims!(0, 0.7)\n    xgrid!(:off)\n    xlabel!(\"\\$ x/l^* \\$\")\nend\n\ngif(bouncing, \"img/bouncing.gif\", fps=30)\n\n┌ Info: Saved animation to \n│   fn = /Users/bubu/projects/comp-phys/content/img/bouncing.gif\n└ @ Plots /Users/bubu/.julia/packages/Plots/qZHsp/src/animation.jl:98\n\n\n\n\n\nWhat you see might be surprising to some, and yet so boringly ordinary. The particle starts falling towards the ground (gravity points to the left in this situation), “reaches” it the classical time \\(\\tau = \\sqrt{\\frac{2x_0}{g}}\\), self-interferes and reflects and starts flying up until it reaches it’s maximum height at a time that coincides with the classical points mass.\nOne thing you won’t see in classical mechanics, is that the shape of the wave packet gets somewhat wonky as time passes. This is because the solutions to Schrödinger’s equation are dispersive, physical and formal implications of which you can read about in plenty of places on the web (e.g. a discussion here). To better see this effect, we can plot the mean expectation value of the position $ < > = < | | > = {j = 1} ^{n{max}} x_j | _j | ^2$ with the standard deviation \\(\\sigma _x = \\left< x^2 \\right> - \\left< x \\right> ^2\\):\n\nx_mean = [abs2.(ψ)' * xs for ψ in eachcol(ψs)]\nx2_mean = [abs2.(ψ)' * xs.^2 for ψ in eachcol(ψs)]\nσx = x2_mean .- (x_mean .* x_mean)\n\nplot(leg=:topleft)\nplot!(ts, x_mean .+ 0.5 .* σx, fill = x_mean .- 0.5 .* σx,\n    alpha=0.2,\n    label=\"\",\n    color = :darkred,\n    width=0)\nplot!(t -> x_classical(t, 15, 1), ts,\n    label=\"\\$ x_{classical} \\$\",\n    color = :grey,\n    width = 2)\nplot!(ts, x_mean,\n    label=\"\\$ \\\\left< \\\\hat{x} \\\\right> \\$\",\n    color = :darkred,\n    width = 2)\nylims!(0, 22)\n\n\n\n\nYou can see that the mean position slowly drifts away from the classical trajectory and the uncertainty in position keeps increasing (as an exercise for you, dear reader, think of a hand-wavy reason for why does the uncertainty change so much specifically around the turning points of each bounce).\nThe nice thing about this problem is that the only free parameters are mass and potential. You can, for example, calculate the mean expectation value of position in the ground state \\(\\left< x \\right> = \\left< \\psi_1 \\right| \\hat{x} \\left| \\psi_1 \\right>\\) and multiply that number by \\(l^*\\) with \\(m = 1.674 \\cdot 10^{-27}\\) kg, which is the mass of the neutron, and compare that to the experimental results (paywalled paper here). Alternatively, you can try to calculate energies of the confined state in 2D electron gases, where you have have a potential \\(W(x) = eFx\\) with \\(e\\) the charge of an electron and \\(F\\) a constant that approximates the slope of the band edge at the interface (for a realistic values you can use those for GaAs, where \\(F \\approx 5\\) MV/m and \\(m = 0.067 m_e\\) with \\(m_e\\) being the mass of a free electron). The energy difference between the ground and first excited state should give you a rough idea of the temperature \\(T = \\frac{E_2 - E_1}{k_B}\\) needed for the electrons behave like a 2D system (\\(k_B\\) is the Boltzmann constant).\nThis is also a good halfway point in today’s topic, so if you want to take a breather, now is the time. If you still haven’t had enough, let’s move on to something that’s tangibly more quantum…\n\n\nIt takes two to tango\nPhysics would be rather boring without interactions. Especially in quantum mechanics, where most of non-interacting problems have been solved. Fortunately, me and others have some job security (haha…) because almost every particle out there has a tendency to interact with others. The interactions can get really complicated, but today we’ll focus on pair interactions between two particles and use the formalism we learned previously. Let the two-particle Hamiltonian be\n\\[ \\hat{H} = \\hat{H}_1 + \\hat{H}_2 + \\hat{H}_{int} ,\\]\nwhere \\(\\hat{H}_{1,2} = \\hat{T}_{1,2} + \\hat{V}_{1,2}\\) are sub-Hamiltonians acting only on the single-particle sub-spaces (their kinetic energies and single-particle potentials) and \\(\\hat{H}_{int}\\) is the Hamiltonian that governs the two-particle interactions and acts on the whole Hilbert space of our system. The basis we’ll use for this Hamiltonian will be a two-particle position basis $| {j_1}, {j_2} > = | {j_1} > | {j_2} > $, which leads to single-particle Hamiltonians with the form \\(\\hat{H}_1 = \\hat{H}_1 \\otimes \\hat{1}_2\\) and \\(\\hat{H}_2 = \\hat{1}_1 \\otimes \\hat{H}_2\\) (if you don’t understand what you’re reading, you should read the second half of the first part of the series). The interaction Hamiltonian will be represented by a potential that depends on the individual particle coordinates \\(V_{int}(x_1, x_2)\\):\n\\[ \\hat{H}_{int} = \\int _0 ^l dx_1 dx_2 \\left| x_1 \\right> \\otimes \\left| x_2 \\right> \\hat{V}_{int} \\left< x_1 \\right| \\otimes \\left< x_2 \\right| .\\]\nIn our basis, this leads to the form:\n\\[\n\\begin{split}\n\\left< \\vartheta_{j_1}, \\vartheta_{j_2} \\right| \\hat{H}_{int} \\left| \\vartheta_{j_1 \\prime}, \\vartheta_{j_2 \\prime} \\right> &= \\int_0 ^l dx_1 dx_2 \\vartheta_{j_1}(x_1) \\vartheta_{j_2}(x_2) \\, V_{int}(x_1,x_2) \\, \\vartheta_{j_1 \\prime} (x_1) \\vartheta_{j_2 \\prime} (x_2) \\\\\n&\\approx V_{int}(x_{j_1}, x_{j_2}) \\int_0 ^l dx_1 dx_2 \\vartheta_{j_1}(x_1) \\vartheta_{j_2}(x_2)\\vartheta_{j_1 \\prime}(x_1) \\vartheta_{j_2 \\prime}(x_2) \\\\\n&= V_{int}(x_{j_1}, x_{j_2})\\delta_{j_1 j_1 \\prime} \\delta_{j_2 \\prime j_2 \\prime},\n\\end{split} \\]\nwhere we assume that the interaction potential is constant around each grid point and that the basis functions \\(\\vartheta(x)\\) are reasonably local and orthogonal - the same assumptions and arguments like in the single-particle case.\nI know from experience that the indexing can get confusing for first-timers (just you wait until we get to many-particle systems in the future), so I’ll elaborate on this a bit more: Because the potential \\(V_{int}\\) depends on two coordinates, one might trick themselves into visualizing the Hilbert space as a plane and quickly lose intuition when doing the inner product \\(\\left< \\vartheta_{j_1}, \\vartheta_{j_2} \\right| \\hat{H}_{int} \\left| \\vartheta_{j_1 \\prime}, \\vartheta_{j_2 \\prime} \\right>\\) or if the system has more than three particles (if you think you can visualize higher than 3-dimensional space, you’re wrong). A better way of thinking about this is by considering that you can flatten the “array” of the two-particle coordinates \\((x_{j_1}, x_{j_2})\\) into a one-dimensional vector, for example like this:\n\\[\n\\begin{bmatrix}\n  (x_{1}, x_{1}) & (x_{1}, x_{2}) & \\dots & (x_{1}, x_{n_{max}}) \\\\\n  (x_{2}, x_{1}) & (x_{2}, x_{2}) & \\dots & (x_{2}, x_{n_{max}}) \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  (x_{n_{max}}, x_{1}) & (x_{n_{max}}, x_{2}) & \\dots & (x_{n_{max}}, x_{n_{max}}) \\\\\n\\end{bmatrix}\n\\mapsto\n\\begin{bmatrix}\n  (x_{1}, x_{1}) \\\\\n  (x_{2}, x_{1}) \\\\\n  \\vdots \\\\\n  (x_{n_{max}}, x_{1}) \\\\\n  (x_{1}, x_{2}) \\\\\n  (x_{2}, x_{2}) \\\\\n  \\vdots \\\\\n  (x_{n_{max}}, x_{2}) \\\\\n  (x_{1}, x_{3}) \\\\\n  (x_{2}, x_{3}) \\\\\n  \\vdots \\\\\n  (x_{n_{max}}, x_{n_{max}})\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\xi_1 \\\\\n\\xi_2 \\\\\n\\vdots \\\\\n\\xi_{n_{max}^2} \\\\\n\\end{bmatrix}.\n\\]\nMapping like this is compatible with the column-major ordering of arrays in Julia and with the definition of Kronecker product that we use. In principle, any bijection \\((x_{j_1}, x_{j_2}) \\mapsto \\xi_m\\) works, but it’s best to use something that follows the computer memory layout to get the best efficiency and to minimize headaches when writing the code (array flattening, reshaping and tensor products usually follow the array ordering too). Many of you probably already encountered this in textbooks or papers, where people say something like “\\(\\left| \\xi_m \\right>\\) where \\(m\\) indexes over all spatial and spin degrees of freedom…” and implicitly assume some “flattening” like the above - it’s much easier to write \\(\\left< \\xi _m \\right| \\hat{H} \\left| \\xi _{m\\prime} \\right>\\) than \\(\\left< x_{j_1}, y_{k_1}, z_{l_1}, \\sigma_{s_1}, x_{j_2}, y_{k_2}, z_{l_2}, \\sigma_{s_2} \\right| \\hat{H} \\left| x_{j_1\\prime}, y_{k_1\\prime}, z_{l_1\\prime}, \\sigma_{s_1\\prime}, x_{j_2\\prime}, y_{k_2\\prime}, z_{l_2\\prime}, \\sigma_{s_2\\prime} \\right>\\), especially if you don’t plan to do numerical solutions.\nWriting the Hamiltonian is easy. The single-particle parts are the same as before, we only need the new interaction potential, which acts on the flattened state vector:\n\nfunction Vint(n_max, W, l)\n    Δ, xs = make_grid(n_max, l)\n    inds = CartesianIndices((n_max, n_max))\n    f(v) = map(zip(v, inds)) do (el, ij)\n        (i, j) = Tuple(ij)\n        el * W(xs[i], xs[j])\n    end\n    LinearMap(f, n_max^2; issymmetric=true)\nend\n\nfunction twoParticleH(n_max, β, α, W, Wint, l)\n    id = LinearMap(identity, n_max; issymmetric=true)\n    m = 1\n    Δ, xs = make_grid(n_max, l)\n    \n    # Single-particle kinetic energy\n    TP1p = Rpr(n_max) * T(n_max, m, l) * Rpr(n_max)\n    TP = kron(TP1p, id) + kron(id, TP1p)\n    \n    # Single-particle potential\n    V1p = V(n_max, W.(xs))\n    VP = kron(V1p, id) + kron(id, V1p)\n    \n    # Two-particle potential\n    VPint = Vint(n_max, Wint, l)\n    \n    TP + α * VP + β * VPint\nend;\n\nI used CartesianIndices to create an iterator over the two single-particle indices \\((j_1, j_2)\\) and multiply the wave function component \\(\\psi_{j_1, j_2}\\) the same way as you do in the single-particle case. If you’re using a different language to program along and don’t have a function to calculate the index for you, you need to just invert the offset calculation, e.g. i = l / n_max and j = l % n_max for zero-indexed, row-major arrays. The full Hamiltonian twoParticleH extends the single-particle parts using a kronecker product with the identity operator (another nice things about LinearOperators is that you can use the kron function as if they were just matrices) and adds all of them together. I also added parameters α and β to tune the strength of the single-particle and interaction potentials.\nWith that, we just need to do the eigendecomposition of the Hamiltonian, which is in this case done by solveTwoParticle function:\n\nfunction solveTwoParticle(n_max, n,  β, α, W, Wint, l)\n    HP2 = twoParticleH(n_max, β, α, W, Wint, l)\n    lobpcg(HP2, false, n)\nend;\n\nWe need to decide on what the interaction of the particles will be. Probably the first thing that comes to everyones mind is \\(V(x_1, x_2) \\propto \\frac{1}{\\left| x_1 - x_2 \\right|}\\) potential such as gravitational or electrostatic interaction, but that’s too long-range to nicely demonstrate what I want to show. I want some form of more localized interaction. The most localized would be contact interaction \\(V(x_1, x_2) = \\delta(x_1 - x_2)\\), but that’s numerically inconvenient (we operate on a grid which causes too many singularities). A good middle ground is the potential\n\\[ V(r) = \\frac{e^{-r}}{r} ,\\]\nwhich many of you might have already encountered. Plasma physicists know it as the Debye screening potential, solid state and condensed matter people use the name Thomas-Fermi potential and particle physicists prefer to call it Yukawa potential. As you would guess by the fact that there exist three different fields that give it a unique name, this potential pops up quite frequently everywhere, which makes it a good example. The most tangible physical motivation for it is from the plasma case, where the charged particles of a plasma try to move around such that electrostatic fields are minimized. This means, that if we put a test charge into the plasma, the plasma reacts and tries to “hide” the electric potential, which is called screening. The Thomas-Fermi case is the same, but you substitute the plasma with the sea of free electrons and the lattice of the material, which might or might not be of quantum nature. In particle physics, the exponential decay is related to the mass of the interaction mediators (this is why the weak nuclear force is short ranged - the W and Z bosons have non-zero mass).\n\npair(x, y) = ifelse(abs(x - y) > 0.09, exp(-abs(x - y)) / abs(x - y), exp(-0.09)/0.09);\n\nYou’ll notice that I had to introduce a cutoff length (which I set to \\(\\Delta = 0.09\\)), to avoid division by zero (the ifelse function is similar to a if statement but evaluates both arguments and the return type is known at compile-time, which prevents some type instability and branch mispredictions). As long as the energies of the particles are smaller than the potential at the cutoff value, this truncated potential is the same as the analytical form.\nWith that, you can take the same system as before: throw two particles into the triangular well, have them interact by the screened interaction, twist it, bop it, solve it…\n\nr = solveTwoParticle(100, 8, 1.0, 1.0, identity, pair, 10);\n\nAnd plot it:\n\nfunction plot_wf(evs, n, n_max, l)\n    Δ, xs = make_grid(n_max, l)\n    ma = findmax(real.(evs[:, n]))[1] / sqrt(Δ)\n    mi = -findmin(real.(evs[:, n]))[1] / sqrt(Δ)\n    ml = max(ma, mi)\n    heatmap(xs, xs,\n        real.(reshape(evs[:, n] ./ sqrt(Δ), (n_max, n_max))),\n        aspect_ratio=:equal,\n        color=:RdGy,\n        leg=:none,\n        clim=(-ml, ml),\n        xlims=(0,8),\n        ylims=(0,8),\n        title=\"n = $n\",\n        xlabel=\"\\$ x_1 / l^* \\$\",\n        ylabel=\"\\$ x_2 / l^* \\$\")\nend\n\nevs = r.X\n\nl = @layout grid(4,2)\nps = []\nfor n in 1:8\n    ps = push!(ps, plot_wf(evs, n, 100, 10.0))\nend\n\nplot(ps..., layout=l, size=(500, 1000))\n\n\n\n\nAnd we can do the same with an attractive interaction between the particles (β = -1):\n\nr = solveTwoParticle(100, 8, -1.0, 1.0, identity, pair, 10)\n\nevs = r.X\n\nl = @layout grid(4,2)\nps = []\nfor n in 1:8\n    ps = push!(ps, plot_wf(evs, n, 100, 10.0))\nend\n\nplot(ps..., layout=l, size=(500, 1000))\n\n\n\n\nSo cool.\nOK, there are few things worth mentioning. First, yes, the calculation takes considerably more time than just the single-particle case. This is for the reasons I already explained previously: The total number of basis elements in a product space is \\(n = \\prod _{k=1} ^{n_{max}} n_k = n_{max}^2\\) and our two-particle Hamiltonian matrix has \\(n_{max}^4\\) elements. Adding even one more particle would increase the number of operations by a factor of 100 (if we keep \\(n_{max}\\) the same), making it too computationally expensive to be worth it.\nThe second thing is that you need to read the plots correctly. This is not a 2D system - the particles live in a one-dimensional world exactly the same as the cases we calculated beforehand. The plot has two axes, because it has two degrees of freedom: coordinates of each particle. The way to read those plots is that the value of \\(\\left| \\psi (x_1, x_2) \\right| ^2\\) is the probability of the particle 1 being at \\(x_1\\) and particle 2 at \\(x_2\\), with \\(x_1\\) and \\(x_2\\) lying on the same line.\nThird thing is obvious, but I will still mention it: You see that with repulsive interaction, the particles prefer to be far away from each other (the probability is low close to the “diagonal” \\(x_1 = x_2\\) of the plot). With attractive interaction, the particles, unsurprisingly, attract and stick together (unless they don’t, more on that in a second).\nThere’s one last thing that sticks out, and it’s also the most important because it’s something that cannot be seen in the classical world: You see the two different colors? I removed the colorbar, so it might be hard to tell, but red and black have opposite signs, a fact which probably deserves it’s own small section:\n\n\nIndistinct and exchangeable\nIn most of our daily experience, you can clearly distinguish objects. That car is that car because it’s there and a goes in a certain direction. You can always tell two classical cows (spherical or not) apart: Daisy is grazing there on the hill and Shelly is at the patch of grass by the barn.\nYou cannot do that in quantum world. Imagine a situation where you have two electrons and at first you measure their positions \\(x_1\\) and \\(x_2\\). After some time (long enough so that you don’t measure the still collapsed eigenstate from the first measurement) you measure them again and get \\(x_1^{\\prime}\\) and \\(x_2^{\\prime}\\). Because you measured positions, you collapsed the state into a position eigenstate and have no idea about the possible trajectories of the particles, so there’s no way of knowing if the electron that was originally at \\(x_1\\) went to position \\(x_1^{\\prime}\\) or \\(x_2^{\\prime}\\) - the identity of the electrons between the measurements got erased. This is a general concept that applies not only to electrons and not only to measurements of positions: All quantum objects of the same type (as defined by their properties like charge, mass, etc.) are fundamentally indistinguishable, where the only identifiable thing is the state a particle can be in, not the particle itself. As we’ll see in this section and many times in the future, this phenomenon has broad impact on basically all of quantum mechanics, but is not found anywhere in classical physics, possibly with the exception of entropy of classical gases, where it is known as the Gibbs paradox.\nNow, let’s take a wave function of two identical particles in position representation \\(\\psi(x_1,x_2)\\), similar to the plots above. Because the particles are identical, switching their places cannot have an impact on the observables, i.e. \\(\\left| \\psi(x_1, x_2) \\right|^2 = \\left| \\psi(x_2, x_1) \\right|^2\\). This means that the two wave functions are the same, up to a global phase rotation:\n\\[ \\psi(x_1, x_2) = e^{i \\alpha} \\psi(x_2, x_1) .\\]\nWe can now use the argument twice to get the equation\n\\[ \\psi(x_1, x_2) = e^{i \\alpha} \\psi(x_2, x_1) = e^{2i \\alpha} \\psi(x_1, x_2) ,\\]\nand that can only hold true if \\(e^{2i \\alpha} = 1\\), which has two solutions: \\(e^{i \\alpha} = 1\\) and \\(e^{i \\alpha} = -1\\). This means that there exist two kinds particles, distinguished by what their wave function does if you exchange them:\n\\[\n\\begin{split}\n\\psi(x_1, x_2) &= \\psi(x_2, x_1) \\\\\n\\psi(x_1, x_2) &= -\\psi(x_2, x_1).\n\\end{split}\n\\]\nThe first type of particles are what we call bosons, which have integer spin and the second fermions, particles with half-integer spin. The Pauli exclusion principle can be easily gleaned from the above equations, because the only solution to two identical fermions at the same place \\(\\psi(x, x) = -\\psi(x, x)\\) is \\(\\psi = 0\\). If you don’t see why the two above equations should say anything about spin, don’t worry, it’s not obvious. That part is handled by something known as the spin-statistics theorem, which I won’t explain more because it’s not important right now and the the Wikipedia article does that more than adequately.\nIf we can write down a particle switching operator \\(\\hat{P}\\), such that \\(\\psi(x_1, x_2) = \\left< x_2, x_1 | \\psi \\right> = \\left< x_1, x_2 \\right| \\hat{P} \\left| \\psi \\right>\\), we can determine the symmetry of any our two-particle states by calculating the expectation value \\(\\left< \\psi \\right| \\hat{P} \\left| \\psi \\right>\\). That part is easy:\n\nfunction P(n_max)\n    f(v) = reshape(v, (n_max, n_max)) |> m -> permutedims(m, (2,1)) |> m -> reshape(m, (n_max^2,))\n    LinearMap(f, n_max^2, issymmetric=true)\nend;\n\nWe just take the flattened vector of position coefficients, reshape it back into the 2D array, transpose and flatten it back. With just two particles, each eigenstate of the Hamiltonian above has to be either symmetric or antisymmetric (\\(\\hat{H}\\) and \\(\\hat{P}\\) commute), as you cane easily convince yourself by meditating on it a little bit, or by directly calculating the expectation values like this:\n\nevs[:, 1]' * P(100) * evs[:, 1]\n\n1-element Array{Float64,1}:\n 1.0000000000067588\n\n\nThere is some floating arithmetic error, which makes the expectation values not exactly \\(\\pm 1\\), but the values are still easy to distinguish.\nThe trickier part comes when we decide to change the order of steps in our solution: What if we only want the (anti)symmetric states and don’t care about the others? Because they are subspaces of the Hilbert space of our Hamiltonian, there should exist a projector that will carve out only the subspace we need. And it does, indeed, exist. The catch is, that this operator is composed of a weighted average of all members of the symmetric permutation group and that bad boy has \\(n_{max}!\\) elements, which a bit too large of a number to work with:\n\nfactorial(big(100))\n\n93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000\n\n\nThe function big turns the number into an arbitrarily large BigInt, because the result is too big to be represented using 64-bit integers.\nThe situation looks dire, but not all hope is lost. A second way is to use the eigendecomposition of the operator \\(\\hat{P}\\). Turns out, that the transposed matrix of all (anti)symmetric eigenvectors will be the projector we’re looking for. That, however, means forming a dense matrix from a large number of eigenvectors - \\(\\frac{n_{max}(n_{max} - 1)}{2}\\) of them for the antisymmetric case and \\(\\frac{n_{max}(n_{max} + 1)}{2}\\) if we’re wanting to project on the symmetric subspace, so the no free lunch principle still applies (it made the lunch cheaper, though).\n\nfunction Πa(n_max)\n    Pop = P(n_max)\n    n = Int(n_max*(n_max - 1)/2)\n    r = lobpcg(Pop, false, n)\n    LinearMap(r.X')\nend\n\nfunction Πs(n_max)\n    Pop = P(n_max)\n    n = Int(n_max*(n_max + 1)/2)\n    r = lobpcg(Pop, true, n)\n    LinearMap(r.X')\nend;\n\nWith these two operators the Hamiltonian of the (anti)symmetric subspace becomes \\(\\hat{H}_{a,s} = \\hat{\\Pi}_{a,s} \\cdot \\hat{H} \\cdot \\hat{\\Pi}^{\\prime}_{a,s}\\):\n\nfunction solveTwoParticle_s(n_max, n,  β, α, W, Wint, l, symmetry=1)\n    Πop = if symmetry > 0 Πs(n_max) else Πa(n_max) end\n    HP2 = twoParticleH(n_max, β, α, W, Wint, l)\n    (lobpcg(Πop * HP2 * Πop', false, n), Πop)\nend;\n\nI have the function solveTwoParticle_s return the solution and the (anti)symetrizer Πop, which we need in order to return the solution back to the full Hilbert space to plot stuff. The eigenvalues of \\(\\hat{P}\\) are highly degenerate, so repeated calls to Πs or Πa need not return the same matrix (the eigenvectors are ordered by magnitude of eigenvalues) and trying to change back into the full basis with a different Πop will scramble the components of the state vector.\n\nr, Πop = solveTwoParticle_s(50, 8, -1.0, 1.0, identity, pair, 10.0, -1);\n\n(I reduced \\(n_{max}\\) to 50 so that I don’t waste too much time on computation). We can now plot the solutions of the anti-symmetrized repelling Hamiltonian and see that they are, indeed, all fermionic states:\n\nevs = Matrix(Πop' * r.X)\n\nl = @layout grid(4, 2)\nps = []\nfor n in 1:8\n    ps = push!(ps, plot_wf(evs, n, 50, 10.0))\nend\n\nplot(ps..., layout=l, size=(500, 1000))\n\n\n\n\nIn the first line, I projected back into the full Hilbert space, so that I can easily plot th the wave functions using the machinery that we already have. The higher excited states are getting a bit too wide for the size of our potential well, so I wouldn’t trust them being quantitatively correct, but all clearly demonstrate that the antisymmetrization works.\nBefore we conclude, there’s one more thing to show. I said that the particle exchange symmetry of many-particle wave functions has huge impact on physics but I haven’t demonstrated it in any way. There will be plenty of examples in the future, especially once we start dealing with solid state and condensed matter systems, but let’s see if there’s is something also in this two-particle well too.\nThe most obvious difference between the symmetric and anti-symmetric case is in the particle relative position. Just by looking at the density wave function plots, you can clearly see that the fermions refuse to exist at the same spot, so any property that depends on their overlap will be affected too. One of these is the total energy of the two-particle system which depends on the overlap through the pair potential. The “closer” they are to each other, the higher (or lower, depending on the sign) the interaction energy will be. To demonstrate that, let’s see what the eigenenergies look like as function of the pair interaction strength \\(\\beta\\).\n\nfunction get_energy(n_max, n, β, W, Wint, l, symmetry=1)\n    r = solveTwoParticle_s(n_max, n,  β, 1.0, W, Wint, l, symmetry)[1]\n    r.λ\nend;\n\nI quickly cobbled together a function that returns only the energies and will map it onto a range of \\(\\beta\\)s:\n\npair2(x, y) = ifelse(abs(x - y) > 0.19, exp(-abs(x - y)) / abs(x - y), exp(-0.19)/0.19)\n\nβs = -5:0.5:5\ne_sym = [get_energy(50, 3, β, identity, pair2, 10.0, 1) for β in βs]'\ne_asym = [get_energy(50, 3, β, identity, pair2, 10.0, -1) for β in βs]';\n\nI also had to include a new pair interaction potential function, to accommodate for the coarser grid (the cutoff is hardcoded because I’m lazy). Let’s plot the energies and see what we see:\n\nplot(βs, vcat(e_sym...), color=1, label=[\"Bosons\" \"\" \"\"])\nplot!(βs, vcat(e_asym...), color=2, label=[\"Fermions\" \"\" \"\"])\nxaxis!(\"\\$ \\\\beta \\$\")\nyaxis!(\"\\$ E/ E^* \\$\")\nplot!(leg=:bottomright)\n\n\n\n\nFor \\(\\beta > 0\\), things don’t change much and there’s little difference between bosons and fermions. That’s because the pair interaction naturally forces the particles apart. On top of that, the exponential dependence of the pair potential introduces a cutoff, beyond which the two particles don’t feel each other, so the situation looks almost like two solid marbles touching, with little dependence on the interaction strength.\nThe more interesting stuff happens in the attractive region (\\(\\beta < 0\\)), where the pair interaction would want to pull the particles close to each other. Bosons have no problem doing this and they keep forming a “tighter bond”, falling into a deeper potential well as the interaction gets stronger. The fermions, however, can get only so close to each other and, because the interaction is short ranged, their bonding energy (which depends on how close they are) changes slowly. If I made \\(\\beta\\) even more negative, the pair interaction would overwhelm the exchange interaction and the the energy dependence would become roughly linear too, but with a smaller slope, because the Pauli exclusion still holds.\nThis behavior will become important once we do Hartree-Fock and density functional theories, where the exchange-driven repulsion creates an exchange-correlation hole and its interplay with the electrostatic interaction will determine the final electron density in molecules and solids.\n\nThat’s it. With what you learned in this and the previous part, you should be able to solve most the undergraduate quantum mechanics, up to non-relativistic model of the hydrogen atom and quantum harmonic oscillators and their dynamics. In the next part, I might return to this naive approach one last time and demonstrate coupling of spin/angular momentum and position degrees of freedom, but that’s about as far as this framework allows us to go. From there, we will have to start introducing much more complicated methods and approximations, but we will be able to calculate properties of much more real, tangible objects than just toy models."
  },
  {
    "objectID": "posts/cmpm3/index.html",
    "href": "posts/cmpm3/index.html",
    "title": "Computational Physics for the Masses Part 3: Solid Stuff",
    "section": "",
    "text": "Guess who’s back? Yeah, it’s been a while, but I’ve been quite busy with…well, things. No matter; the show is going on and today we finally dive into “real” stuff in computational quantum mechanics, more specifically, solid state physics. This part will also be showing some more big-boy practices in programming, as today’s code will serve as a base for future parts of this series. It might mean that some parts will look a bit too verbose and not flow as nicely with the prose. But trust me, it will pay back in the future, so just follow along, there’s also numerical physics in this part. I promise.\nReal quick, let’s get the needed packages out of the way:\nI start using package environments for individual parts of the series so that I don’t pollute my global package list with cruft that I don’t need outside of this blog. This is so that package versions can be resolved with minimal conflict and I avoid dependency hell within my work.\nJust dense linear algebra will carry us almost all the way today, so there’s not much in here. Colors contains some types and definitions for GLMakie, which is a backend for the Makie plotting package that I got to like more than Plots.jl (there are a few lines at the bottom so that the default colors fit the theme of the blog, you can safely skip those). Unitful is a package to use and transform physical units and RangeHelpers gives a few quality-of-life improvements to linear ranges found in the standard library."
  },
  {
    "objectID": "posts/cmpm3/index.html#digging-deeper",
    "href": "posts/cmpm3/index.html#digging-deeper",
    "title": "Computational Physics for the Masses Part 3: Solid Stuff",
    "section": "Digging deeper",
    "text": "Digging deeper\nThe above plot looks good but the physical interpretation is lacking. The band diagram looks like it does because I claim that there are only two equivalent orbitals participating, which is often simply stated in textbooks and might seem suspect based on the fact that carbon has more than one valence electron. While I’m all for dictating how nature should do things, there’s a certain beauty to making predictions that work not because they do, but because they should. To show why graphene should have these bands, let’s drop the empirical approach and try to do a first principles-ish calculation by doing tight-binding with real carbon atomic orbitals.\nLet’s first look at the orbital wavefunctions. Central potential problems, like those of hydrogen-like atoms, are usually solved in spherical coordinates, so I’ll quickly throw together some helper functions to convert cartesian coords:\n\nfunction to_polar(r)\n    x,y,z = r\n    r = norm(r)\n    θ = ifelse(r != 0, acos(z/r), 0.0)\n    φ = if x > 0\n        atan(y/x)\n    elseif x < 0\n        atan(y/x) + π\n    else\n        π/2\n    end\n    [r,θ,φ]\nend;\n\nThe general form for the eigenfunctions is a product of Laguerre polynomials and spherical harmonics but because we’re limiting ourselves just to the \\(n = 2\\) states, I can write down the specific form in our atomic units: \\(\\phi(\\vec{r})_{2s} = \\frac{1}{4\\sqrt{2\\pi}} (2 - r) e^{-r/2}\\), \\(\\phi(\\vec{r})_{2p,m_l=0} = \\frac{1}{4\\sqrt{2\\pi}} r e^{-r/2} \\cos{(\\theta)}\\) and \\(\\phi(\\vec{r})_{2p,m_l=\\pm 1} = \\frac{\\mp 1}{8\\sqrt{\\pi}} r e^{-r/2} \\sin(\\theta) e^{\\pm i \\varphi}\\), where I separated the cases of the magnetic quantum number \\(m_l=0\\) and \\(m_l=\\pm1\\) (the choice for identification of the latter is arbitrary, you just need the two \\(m_l=\\pm1\\) have opposite signs so that they’re orthogonal).\n\nfunction ψ2s(r)\n    r,θ,φ = to_polar(r)\n    1/(4*sqrt(2π)) * (2 - r) * exp(-r/2) + 0im\nend;\n\n\nfunction ψ2p(r, ml)\n    r,θ,φ = to_polar(r)\n    ifelse(ml == 0,\n        1/(4*sqrt(2π)) * r * exp(-r/2) * cos(θ) + 0im,\n        -ml/(8*sqrt(2π)) * r * exp(-r/2) * sin(θ) * exp(1im * ml * φ)\n    )\nend;\n\nThe \\(m_l = 0\\) orbital is recognizably the \\(p_z\\) orbital you know from textbooks:\n\nfoo = [ψ2p([i,j,k], 0) |> abs2 for i ∈ -3:0.1:3, j ∈ -3:0.1:3, k ∈ -5:0.1:5]\ncontour(foo, levels=1)\n\n\n\n\nThe other two might not be what you expect if your knowledge of atomic orbitals comes from chemistry:\n\nfoo = [ψ2p([i,j,k], 1) |> abs2 for i ∈ -5:0.1:5, j ∈ -5:0.1:5, k ∈ -4:0.1:4]\ncontour(foo, levels=1)\n\n\n\n\nThat’s not the dumbbell shape that you’re usually shown. It is the correct solution to the radial Schroedinger equation, so why are we getting a donut? Well, it’s because of conventions. If the eigenfunctions \\(\\left| p_{-1} \\right>,\\left| p_{1} \\right>,\\left| p_{0} \\right>\\) are solutions to the Schroedinger equation (which they are, that’s why they’re eigenfunctions), then so is any of their linear combinations. Because the purely real dumbbell-shaped functions are more convenient when discussing chemical bonds and symmetries of crystals, it is customary to rotate the \\(\\{\\left| p_{m_l} \\right> \\}_{m_l={-1,1,0}}\\) basis into a different basis \\(\\{ \\left| p_{r} \\right> \\}_{r={x,y,z}}\\), where \\(\\left| p_{x} \\right>\\) and \\(\\left| p_{y} \\right>\\) (also known as cubic harmonics) are linear combinations of \\(\\left| p_{-1} \\right>\\) and \\(\\left| p_{1} \\right>\\):\n\nfunction ψ2pz(r)\n    r,θ,φ = to_polar(r)\n    1/(4*sqrt(2π)) * r * exp(-r/2) * cos(θ) + 0im\nend\n\nfunction ψ2px(r)\n    r,θ,φ = to_polar(r)\n    1/(4*sqrt(2π)) * r * exp(-r/2) * sin(θ) * cos(φ) + 0im\nend\n\nfunction ψ2py(r)\n    r,θ,φ = to_polar(r)\n    1/(4*sqrt(2π)) * r * exp(-r/2) * sin(θ) * sin(φ) + 0im\nend;\n\nNow the \\(p_x\\) and \\(p_y\\) orbitals have the shape you’re used to:\n\nfoox = [ψ2px([i,j,k]) |> abs2 for i ∈ -5:0.1:5, j ∈ -3:0.1:3, k ∈ -3:0.1:3]\ncontour(foox, levels=1)\n\n\n\n\nWhen thrown together, they form a nice symmetric structure:\n\nf = Figure()\nl = 3.4\nz = 3.4\nhybp = Axis3(f[1, 1], aspect=(1.,1.,z/l), elevation=0.2π, azimuth=0.2π)\n\nfoox = [ψ2px([i,j,k]) |> abs2 for i ∈ -l:0.1:l, j ∈ -l:0.1:l, k ∈ -z:0.1:z]\nfooy = [ψ2py([i,j,k]) |> abs2 for i ∈ -l:0.1:l, j ∈ -l:0.1:l, k ∈ -z:0.1:z]\nfooz = [ψ2pz([i,j,k]) |> abs2 for i ∈ -l:0.1:l, j ∈ -l:0.1:l, k ∈ -z:0.1:z]\nfoos = [ψ2s([i,j,k]) |> abs2 for i ∈ -l:0.1:l, j ∈ -l:0.1:l, k ∈ -z:0.1:z]\ncontour!(foox, levels=[4e-3])\ncontour!(fooy, levels=[4e-3])\ncontour!(fooz, levels=[4e-3])\ncontour!(foos, levels=[4e-3])\nhidedecorations!(hybp, label=false)\ncurrent_figure()\n\n\n\n\nWith one obvious issue: That is not the trigonal symmetry of graphene. What actually happens, is that the \\(2s\\), \\(2p_x\\), and \\(2p_y\\) orbitals undergo hybridization: By the same argument used for the formation of the cubic harmonics, you can mix the one \\(2s\\) and two \\(2p\\) orbitals to form a trigonal structure with lopsided orbitals extending outwards from the center, rotated by 120 degrees:\n\nψ1(r) = 1/√(3)*ψ2s(r) + 1/√(6) * ψ2px(r) + 1/√(2) * ψ2py(r)\nψ2(r) = 1/√(3)*ψ2s(r) + 1/√(6) * ψ2px(r) - 1/√(2) * ψ2py(r)\nψ3(r) = 1/√(3)*ψ2s(r) - 2/3 * ψ2px(r)\n\nl = 3\nz = 2\n\nf = Figure()\nhybp = Axis3(f[1, 1], aspect=(1.,1.,l/z), elevation=0.2π, azimuth=0.2π)\nl = 1\nfoo1 = [ψ1([i,j,k]) |> abs2 for i ∈ -l:0.1:l, j ∈ -l:0.1:l, k ∈ -z:0.1:z]\nfoo2 = [ψ2([i,j,k]) |> abs2 for i ∈ -l:0.1:l, j ∈ -l:0.1:l, k ∈ -z:0.1:z]\nfoo3 = [ψ3([i,j,k]) |> abs2 for i ∈ -l:0.1:l, j ∈ -l:0.1:l, k ∈ -z:0.1:z]\nfooz = [ψ2pz([i,j,k]) |> abs2 for i ∈ -l:0.1:l, j ∈ -l:0.1:l, k ∈ -z:0.1:z]\ncontour!(foo1 ./ maximum(foo1), levels=[0.6])\ncontour!(foo2 ./ maximum(foo2), levels=[0.6])\ncontour!(foo3 ./ maximum(foo3), levels=[0.6])\ncontour!(fooz, levels=[5e-3])\nhidedecorations!(hybp, label=false)\ncurrent_figure()\n\n\n\n\nBecause the 2-dimensional graphene sheet doesn’t have a translational symmetry along the z-axis, the \\(2p_z\\) orbitals keep their shape. This can be seen from calculating the overlap integrals \\(\\left< 2p_z | \\phi \\right>\\) which are equal to 0 for \\(\\left| \\phi \\right> = \\left| 2p_{x,y} \\right>\\) or \\(\\left| 2s \\right>\\) at the same site because they are orthogonal, and also equal to zero when overlapping from different sites, because everything else lies in plane \\(z=0\\), over which the \\(|2p_z>\\) is anti-symmetric (any contribution from \\(z>0\\) will be exactly cancelled by contributions from \\(z<0\\)).\nThis is known as \\(sp^2\\) hybridization, named so because 2 of the \\(2p\\) orbitals participate in it (if we were working with a diamond, all orbitals would undergo a \\(sp^3\\) hybridization, forming a tetrahedral structure).\nBut solving the whole problem by just symmetry arguments is not what I want to do here. Let’s pretend we don’t know anything about hybridization and prepare the crystal with the 4 regular cubic harmonics sitting at each site:\n\norbs = [\n    (:s, [2/3, 1/3]),  #1\n    (:s, [1/3, 2/3]),  #2\n    ###\n    (:px, [2/3, 1/3]), #3\n    (:px, [1/3, 2/3]), #4\n    ###\n    (:py, [2/3, 1/3]), #5\n    (:py, [1/3, 2/3]), #6\n    ###\n    (:pz, [2/3, 1/3]), #7\n    (:pz, [1/3, 2/3]), #8\n];\n\n\ngrorbs = Crystal(\n    Lattice(2.468Å, 2.468Å, 120),\n    UnitCell([first(orb) for orb ∈ orbs], (last(orb) for orb ∈ orbs)...)\n);\n\nNow comes the fun part. There are 48 non-conjugate hops and 2 different on-site energies (one for \\(2s\\) and \\(2p\\) at each site), leading to a Hamiltonian with 50 free parameters, which is about 40 too many for practical purposes. Fortunately, the integrals of type \\(\\left< \\phi_1 \\right| \\hat{H} \\left| \\phi_2 \\right>\\) can be nicely factored if the \\(\\left| \\phi \\right>\\)s are cubic harmonics, as was shown by Slater and Koster (non-paywalled article here). Looking at the abridged list at Wikipedia, you see that we need only two sets of four numbers and the displacement vector of the orbitals corresponding to each hop. The values are for integrals between two \\(s\\)-type orbitals labeled \\(ss\\sigma\\), an \\(s\\)-type and \\(p\\)-type orbital labeled \\(sp\\sigma\\), two \\(p\\)-type orbitals in the same plane (\\(pp\\sigma\\)), and two \\(p\\)-type orbitals extending in planes perpendicular to each other (\\(pp\\pi\\)). This is known as the Slater-Koster (SK) parametrization and it reduces the number of free parameters down to 10 in our case.\nAnd this is where the tight-binding methods really shine. You can do a computationally heavy calculation (e.g., DFT) or measurement (say, photoemission) of the band energies at a few select momentum values and then fit the parameters to these known energy values. After that, you can quickly churn out the band structure all across the k-space, or add some more physics to it if you so please.\nTo nobody’s surprise, this was already done for graphene (non-paywall), so I can just extract the numbers:\n\nhopping_ints = Dict(\n    :ssσ => -5.729eV,\n    :spσ => 5.618eV,\n    :ppσ => 6.050eV,\n    :ppπ => -3.070eV\n)\nonsite_es = Dict(\n    :s => -8.37eV,\n    :p => 0eV\n)\noverlap_vals = Dict(\n    :ssσ => 0.102,\n    :spσ => -0.171,\n    :ppσ => -0.377,\n    :ppπ => 0.070\n);\n\nWith that, we start feeding our code the values (I use pattern matching with the @match macro to avoid horribly nested if-else statements):\n\ngr2hops = Hoppings(grorbs)\n\ndir_cos(r) = r ./ norm(r)\nfor hop ∈ unique_neighbors(grorbs)\n    orb_types = (orbs[hop.i][1], orbs[hop.j][1])\n    l, m = dir_cos(grorbs.lattice.R * hop.δ)\n    γ = @match orb_types begin\n        (:s, :s)   => hopping_ints[:ssσ]\n        (:s, :px)  => l * hopping_ints[:spσ]\n        (:s, :py)  => m * hopping_ints[:spσ]\n        #(:s, :pz) => 0.\n        (:px, :s)  => l * hopping_ints[:spσ]\n        (:px, :px) => l^2 * hopping_ints[:ppσ] + (1-l^2) * hopping_ints[:ppπ]\n        (:px, :py) => l*m * (hopping_ints[:ppσ] - hopping_ints[:ppπ])\n        #(:px, :pz) => 0.\n        (:py, :s)  => m * hopping_ints[:spσ]\n        (:py, :px) => l*m * (hopping_ints[:ppσ] - hopping_ints[:ppπ])\n        (:py, :py) => m^2 * hopping_ints[:ppσ] + (1-m^2) * hopping_ints[:ppπ]\n        #(:py, :pz) => 0.\n        #(:pz, :s)  => 0.\n        #(:pz, :px) => 0.\n        #(:pz, :py) => 0.\n        (:pz, :pz) => hopping_ints[:ppπ]\n        _ => 0eV\n    end\n    addhop!(gr2hops, γ, hop.i, hop.j, hop.δ)\n\n    s = @match orb_types begin\n        (:s, :s)   => overlap_vals[:ssσ]\n        (:s, :px)  => l * overlap_vals[:spσ]\n        (:s, :py)  => m * overlap_vals[:spσ]\n        #(:s, :pz) => 0.\n        (:px, :s)  => l * overlap_vals[:spσ]\n        (:px, :px) => l^2 * overlap_vals[:ppσ] + (1-l^2) * overlap_vals[:ppπ]\n        (:px, :py) => l*m * (overlap_vals[:ppσ] - overlap_vals[:ppπ])\n        #(:px, :pz) => 0.\n        (:py, :s)  => m * overlap_vals[:spσ]\n        (:py, :px) => l*m * (overlap_vals[:ppσ] - overlap_vals[:ppπ])\n        (:py, :py) => m^2 * overlap_vals[:ppσ] + (1-m^2) * overlap_vals[:ppπ]\n        #(:py, :pz) => 0.\n        #(:pz, :s)  => 0.\n        #(:pz, :px) => 0.\n        #(:pz, :py) => 0.\n        (:pz, :pz) => overlap_vals[:ppπ]\n        _ => 0\n    end\n    addoverlap!(gr2hops, s, hop.i, hop.j, hop.δ)\nend\n\nfor (i, orb) ∈ enumerate(orbs)\n    orb_type = orb[1]\n    μ = @match orb_type begin\n        :s => onsite_es[:s]\n        _  => onsite_es[:p]\n    end\n    addonsite!(gr2hops, μ , i)\nend;\n\nThe symmetry arguments creep in here again: almost every value related to the \\(p_z\\) orbitals, whether it’s hopping or overlap, is equal to 0 because the \\(z\\)-distance of all orbitals is equal to 0. The only non-zero contribution is from the \\(pp\\pi\\)-terms of \\(p_z\\) orbitals as they are aligned in a plane perpendicular to the \\(xy\\)-directions.\nWith the parameters done, now I just construct the k-space trajectory, such that we can compare it to the published stuff linked above:\n\ngrks2 = kpath([\n        :K => [1/3,1/3],\n        :Γ => [0,0],\n        :M => [1/2,0],\n        :K => [1/3,1/3]\n        ], 0.005);\n\nAnd solve, this time keeping the eigenvalues and eigenvectors (which I normalize):\n\ngr2tbH(k) = tbH(k, gr2hops)\ngr2sols = gr2tbH.(grks2.path) .|> x->eigen(x...)\ngr2es = [eig.values for eig ∈ gr2sols]\ngr2vecs = [mapslices(v->v./norm(v), eig.vectors, dims=1) for eig ∈ gr2sols];\n\nI could just plot the bands the same boring way as before but, because this is the money plot of this part, I’ll get a bit fancier with it.\nThe elements of the eigenvectors (or rather their squares) tell us how much of each orbital is mixed into the band at a given k-value, so we can tell if a given band has more \\(2p_{x/y/z}\\) or \\(2s\\) character. Because this is not my first rodeo (and I read the above-linked papers), I’ll color-code the bands by three different categories: the redder the band, the more \\(2s\\)-like it is at that momentum, \\(2p_{x/y}\\)-like bands are green and bands with \\(2p_z\\) character will be blue:\n\nfig_gr2 = Figure()\nax_gr2 = Axis(fig_gr2)\nax_gr2.xticks = ([p.second for p ∈ grks2.ppoints], \n                [string(p.first) for p ∈ grks2.ppoints])\nax_gr2.yticks = -0.7:0.1:0.5\nax_gr2.ylabel = \"E [Ha]\"\n\nxlims!(ax_gr2, (0, grks2.plength[end]))\nylims!(ax_gr2, (-0.75,0.55))\nhideydecorations!(ax_gr2, ticks=false, ticklabels=false, label=false)\n\nfor n ∈ 1:length(gr2es[1])\n    for i ∈ 1:length(grks2.plength)\n    pzc = abs2.(gr2vecs[i])[:,n] ⋅ [0,0,0,0,0,0,1.,1.] # sum contributions from pz orbitals\n    sc = abs2.(gr2vecs[i])[:,n] ⋅ [1.,1.,0,0,0,0,0,0] # sum contributions from s orbitals\n    pxyc = abs2.(gr2vecs[i])[:,n] ⋅ [0,0,1.,1.,1.,1.,0,0] # sum contributions from px/y orbitals\n    scatter!(grks2.plength[i], gr2es[i][n];\n            color=RGB(sc,pxyc,pzc), markersize=3)\n        end\n    end\n\nfig_gr2[1,1] = ax_gr2\ncurrent_figure()\n\n\n\n\nAnd here we are - real, raw, physics. You see that the \\(2s\\) and \\(2p_{x/y}\\) bands do mix, while the \\(2p_z\\) bands stay untouched, which is exactly the \\(sp^2\\) hybridization in action and the reason why you often see graphene discussed only in terms of two orbitals. The electrons fill up the states up to the Fermi level (E = 0) and the only two bands close to that value are the \\(2p_z\\) orbitals, which means that their linear dispersion around the \\(K\\)-point dominates low-energy physics (the usual solid state and condensed matter conditions). Because they don’t interact with the rest, one can pluck them out of the Hamiltonian and deal with just the 2-state system.\nThe last two small things to discuss are the amount of mixing and the energy asymmetry of the \\(2p_z\\) bands. You see that the states close to the \\(\\Gamma\\)-point (lowest momentum) are very symmetric in real space. They either belong to the rotationally symmetric \\(2s\\) band or to the degenerate \\(2p_{x,y}\\) bands that would probably like to form the also symmetric \\(2p_{m_l = \\pm,1}\\) orbitals. This is because the highly delocalized low-momentum states feel a mostly uniform background due to their spatial extent. But as the states disperse, the lattice symmetry does its thing and forces the hybridization so that they extend along the space between the atomic cores which minimizes their potential energy.\nAs for the \\(2p_z\\) orbitals, we didn’t do it entirely right the first time. In the model graphene, we assumed that both states don’t have any structure to them and that they have the same energy. In real graphene that’s not true. The \\(2p_z\\) orbitals have polarity, where the wavefunction is negative at one lobe and positive at the other, which affects the energy of states that are constructed by linear combinations of the \\(2p_z\\) orbitals. Because we’re in a crystal with an infinite amount of atoms, it’s not as intuitively obvious, but the effect at play is very similar to that of the simple splitting into bonding and anti-bonding orbitals in molecules, where delocalization of the state reduces the energy of the parallel one. The case at hand is, of course, still somewhat artificial because the value of the overlap integral can have an arbitrary sign or magnitude. The real physics that’s driving the electron-hole asymmetry in graphene is much more complicated, but it manifests itself in this simple manner when fit to better data.\n\nPacking it up\nEverything works, but there’s a lot of boilerplate code involved in getting from the definition of the crystal to plotting the bands. To eliminate the headaches of dealing with multiple structures and functions with arcane signatures, I’ll finish by creating a kind of an interface.\nThe user wants to know how to define a problem and have it solved, maybe plotted. They don’t want to deal with the inner workings of the solution. For that, I define a type called Solution which will hold all the important data, no matter how the solution was acquired (and have accessor functions to retrieve it, in case I change the inner structure of the Solution in the future):\n\nstruct Solution\n    ks::Vector{Union{<:Real, Vector{<:Real}}}\n    kp::Vector{<:Real}\n    kl::Vector{Pair{Symbol, <:Real}}\n    es::Vector{Vector{Real}}\n    evs::Vector{Matrix{Complex}}\nend\n\nevals(s::Solution) = s.es\nevecs(s::Solution) = s.evs\nkvecs(s::Solution) = s.ks;\n\nAnd a function that will give the solution. First, a catch-all method that errors if the problem definition doesn’t yield a solution:\n\nfunction solve(p)\n    t = typeof(p)\n    ArgumentError(\"$t has no solution implemented.\")\nend;\n\nThen, I’ll conjure a type that determines the problem to be solved. The solution in this case is the tight-binding method. Because Hoppings are relatively free-form in their construction, they have to be generated externally, either with the help of provided helper functions or entirely by hand.\n\nstruct TightBindingProblem\n    hops::Hoppings\n    ks::Vector{Union{<:Real, Vector{<:Real}}}\n    kp::Vector{<:Real}\n    kl::Vector{Pair{Symbol, <:Real}}\nend\n\nfunction TightBindingProblem(hops::Hoppings, kpositions::Vector, kstep::Real)\n    k = kpath(kpositions, kstep)\n    TightBindingProblem(hops, k.path, k.plength, k.ppoints)\nend;\n\nAnd finally, a method of solve that will handle the tight-binding problem.\n\nfunction solve(p::TightBindingProblem)::Solution\n    h(k) = tbH(k, p.hops)\n    sols = gr2tbH.(p.ks) .|> x->eigen(x...)\n    Solution(\n        p.ks,\n        p.kp,\n        p.kl,\n        [eig.values for eig ∈ sols],\n        [mapslices(v->v./norm(v), eig.vectors, dims=1) for eig ∈ sols]\n    )\nend;\n\nThat is technically all that’s needed, but we can provide a convenience function that will plot the bands we just solved for, which will work with any method, as long as it outputs our Solution:\n\nfunction plotSolution(s::Solution)\n    fig = Figure()\n    ax = Axis(fig)\n    ax.xticks = ([p.second for p ∈ s.kl], \n                 [string(p.first) for p ∈ s.kl])\n    ax.ylabel = \"E [Ha]\"\n\n    xlims!(ax, (0, s.kp[end]))\n    hideydecorations!(ax, ticks=false, ticklabels=false, label=false)\n    for n ∈ 1:length(s.es[1])\n        lines!(s.kp, [e[n] for e ∈ s.es])\n    end\n    fig[1,1] = ax\nend;\n\nAnd this is how it works in practice. I’ll define the TB problem, using the hoppings I already made before:\n\ngrprob = TightBindingProblem(gr2hops, [\n    :K => [1/3,1/3],\n    :Γ => [0,0],\n    :M => [1/2,0],\n    :K => [1/3,1/3]\n], 0.005);\n\nSolve it and pipe it straight into the plotting routine:\n\nsolve(grprob) |> plotSolution\nylims!(current_axis(), (-0.75,0.55))\ncurrent_axis().title=\"Graphene\"\ncurrent_figure()\n\n\n\n\nNice, convenient and, depending on how many hoppings you need, short.\n\nAnd here we are, the very basics of solid state physics are covered. There are a few more single-particle methods that I might cover in the next part, although they are generally more niche and often used just as precursors to many-particle problems, so I might skip straight to interacting systems and delve into actual condensed matter physics.\nBetween now and the next part of this series, a large part of today’s code (functions and types related to crystals and the solution interfaces and methods) will be put into a library that we’ll then start extending and using. That means this will be the last truly self-contained post, but that is a sacrifice that has to be made."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Bit Correlated",
    "section": "",
    "text": "Facelift\n\n\n\n\n\nOr: Why does the website look different?\n\n\n\n\n\n\nJan 19, 2023\n\n\nTomas Polakovic\n\n\n\n\n\n\n  \n\n\n\n\nComputational Physics for the Masses Part 3: Solid Stuff\n\n\n\n\n\n\n\nJulia\n\n\nQuantum mechanics\n\n\nSolid state physics\n\n\n\n\nFirst delve into solid state physics: Crystals, symmetries, electronic bands. We explore basic computational methods, namelly the nearly free electron model and the tight binding method.\n\n\n\n\n\n\nOct 18, 2022\n\n\nTomas Polakovic\n\n\n\n\n\n\n  \n\n\n\n\nComputational Physics for the Masses Part 2: Back to Real Space\n\n\n\n\n\n\n\nJulia\n\n\nQuantum mechanics\n\n\nBasics\n\n\n\n\nWe continue expanding the basic language of quantum meachanics and solve problems of one and two particles in infinite potential wells.\n\n\n\n\n\n\nJan 23, 2020\n\n\nTomas Polakovic\n\n\n\n\n\n\n  \n\n\n\n\nComputational Physics for the Masses Part 1: Spinning and Sloshing in Atoms\n\n\n\n\n\n\n\nJulia\n\n\nQuantum mechanics\n\n\nBasics\n\n\n\n\nOur first jump into quantum mechanics.\n\n\n\n\n\n\nOct 3, 2019\n\n\nTomas Polakovic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputational Physics for the Masses Part 0: Hello, World!\n\n\n\n\n\n\n\nQuantum mechanics\n\n\nBasics\n\n\n\n\nIntroduction into the series.\n\n\n\n\n\n\nOct 3, 2019\n\n\nTomas Polakovic\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Tom. I’m a wannabe physicist and amateur digital artist. Around here, you’ll be able to find stuff related to my work, hobbies and combinations of thereof."
  }
]